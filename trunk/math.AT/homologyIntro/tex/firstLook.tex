%%
%% firstLook.tex
%% 
%% Made by Alex Nelson
%% Login   <alex@tomato>
%% 
%% Started on  Mon Aug 31 20:25:55 2009 Alex Nelson
%% Last update Mon Aug 31 20:25:55 2009 Alex Nelson
%%

\marginnote{\begin{center}\includegraphics{img/img.0}\\ \refstepcounter{figure} {\sc Figure~\thefigure}\end{center}\label{fig:img0}}
Consider a graph, which as we know consists of vertices
(``dots'') and edges connecting the vertices. For our purposes we
will work with directed edges with labels. We can see an example
of such a graph in figure \ref{fig:img0}. This is a simple graph
consisting of two vertices $v_{1}$ and $v_{2}$, and a single edge
from $v_{1}$ to $v_{2}$ which we denote by $e_{1}$. We can
consider a slightly more complicated but more interesting example
by adding more edges. We see in figure \ref{fig:img1} that there
are now 3 edges from $v_{1}$ to $v_{2}$. We can consider some
path by ``composing'' the edges shown in the figure. That is, we
can go along $e_{1}$ from $v_{1}$ to $v_{2}$, then go along
$e_{3}$ from $v_{2}$ to $v_{1}$. We will denote this particular
path by $e_{1}e_{3}^{-1}$. \marginpar{Inverse of edges $e_{i}^{-1}$ runs counter to its orientation}
Whenever an edge is ``inverted'', it really means we ``go against
the grain'' (travel counter to its orientation).

The algebraic structure of all paths in a graph is something
called the ``free group generated by the edges $e_{i}$''. It
basically means we consider all paths, even ones like
\begin{equation}%\label{eq:}
e_{1}^{-1}e_{2}e_{3}^{-1}e_{1}e_{2}^{-1}\cdots
\end{equation}
We cannot permute terms, because that would \emph{change the path taken}.
This means, in the language of abstract algebra, the free group
generated by the edges of the graph is \emph{non-Abelian}
(noncommutative). This is a rich and beautiful (and complicated!)
structure. We will often simplify things by Abelianization.

Why do this? Well, consider the path $e_{1}e_{2}^{-1}$ --
travelling to $v_{2}$ across $e_{1}$, then back to $v_{1}$ by
$e_{2}$. Compare this to $e_{2}^{-1}e_{1}$ -- travelling to
$v_{1}$ by $e_{2}$, then travelling back to $v_{2}$ via
$e_{1}$. What's the difference between these two paths? Where we
end up! That is, \emph{the base point.} If we are willing to lose
this information, the paths become ``Abelianized'' -- we can go
forwards from start to finish, or backwards from finish to start,
and identify the two paths as ``the same''. We will call such
closed paths \textbf{``pre-cycles''}\footnote{I am unaware if this term
  is defined elsewhere in math or not, I am defining it as an
  \emph{ad hoc} notion that will make the intuition of cycles clear.}.
It is important to emphasize that these paths are ``loops'', they
begin and end at the same vertex.

\marginnote{\vspace{-8pc}\begin{center}\includegraphics{img/img.1}\\ \refstepcounter{figure}
  {\sc Figure~\thefigure}\label{fig:img1}\end{center}} 

Having thus ``Abelianized'' the situation, we can switch from
using multiplication as the binary operator to using addition
(this is ``syntactic sugar'' --- everyone has the intuition that
addition is commutative! Why not abuse this intuition and use
addition for all Abelian binary operators?). Inverses are mapped
to negatives. In a sense this is sort of like taking the natural
logarithm of the situation. We can consider integer linear
combinations of the edges. These linear combination of edges are
called \textbf{``chains''}.

\marginnote{\vspace{-4pc}
\begin{center}\includegraphics{img/img.2}\\ \refstepcounter{figure}
  {\sc Figure~\thefigure}\label{fig:img2}\end{center}} 

Consider the situation with 2 vertices and 4 edges %(I do not have
%them drawn, it's an exercise for the reader!). 
as drawn in figure \ref{fig:img2}. We can have a
chain be ``grouped'' into terms, or ``decomposed'' if you like
into precycles. Consider
\begin{equation}\label{eq:abelianizedChainsProblem}
(e_{4}-e_{3})+(e_{2}-e_{1})=(e_{4}-e_{1})+(e_{2}-e_{3})
\end{equation}
this is mathematically valid, since we have Abelianized the
situation. We don't really want to say ``The left hand side of eq
\eqref{eq:abelianizedChainsProblem} is a different chain than the
one on the right due to the grouping of terms (`due to different
decompositions into prechains')''. It would entirely defeat the
purpose of having Abelianized the problem in the first place! We
introduce now the notion of a \textbf{``Cycle''} as a precycle
with at least one decomposition. This is a more rigorous
intuition to have, since the notion of decomposition is seemingly
more rigorous.

But what does this mean? It's a seemingly empty addition to the
notion of a precycle. It means that if we consider the chain as a
path that we travel in time (first we go to vertex $v_{i}$ by the
edge $e_{j}$, then \ldots), \emph{we enter the vertex as many times as we leave the vertex.}
That's the intuition behind it, at any rate. So in figure
\ref{fig:img1}, there are thus 2 cycles: $e_{1}-e_{2}$ and
$e_{2}-e_{3}$ (or $e_{1}-e_{3}$, any two of these three would
suffice --- we can write the remaining one as a linear
combination of these two). The condition, really, can be put more
algebraically for figure \ref{fig:img1}: consider some arbitrary
chain $k^{1}e_{1}+k^{2}e_{2}+k^{3}e_{3}$. It enters the vertex
$v_{2}$ precisely $k^{1}+k^{2}+k^{3}$ times since all the edges
\emph{end} in $v_{2}$. The chain leaves the vertex $v_{1}$
precisely $-k^{1}-k^{2}-k^{3}$ times (since all the edges
\emph{leave} $v_{2}$ it costs us a sign). So if the chain is a
cycle, we demand that
\begin{equation}%\label{eq:}
k^{1}+k^{2}+k^{3} = 0
\end{equation}
which says that the number of times it enter $v_{1}$ is equal to
the number of times it leaves $v_{1}$ (i.e. enters $v_{2}$).

Now what are we really doing here? What's the underlying process
that we are doing?\marginpar{Free Abelian group is just $\mathbb{Z}^{n}$ equipped with addition}
Well, we have the free Abelian group generated
by the edges $e_{1}$, $e_{2}$, and $e_{3}$. Lets call this group
$C_{1}$. We also have the free Abelian group generated by the
vertices $v_{1}$ and $v_{2}$. Lets denote this group by
$C_{0}$. We have some intuition, now, of ``number of times an
edge enters a vertex'' and ``number of times an edge leaves a
vertex''. We can make this more rigorous by turning it into a
mapping (``group homomorphism'') 
\begin{equation}%\label{eq:}
\partial_{0}:C_{1}\to C_{0}
\end{equation}
which precisely maps each edge to its target minus its source. We
can make these intuitions rigorous too by creating similar
mappings $t:C_{1}\to C_{0}$ and $s:C_{1}\to C_{0}$, but hopefully
it should be obvious which vertex is the head of the edge and
which vertex is the tail. We can specify the behavior of the
chain $k^{1}e_{1}+\ldots+k^{3}e_{3}$ by
\begin{equation}%\label{eq:}
\partial_{0}:k^{1}e_{1}+\ldots+k^{3}e_{3}\mapsto k^{1}\Big(t(e_{1})-s(e_{1})\Big)
+ \ldots + k^{3}\Big(t(e_{3})-s(e_{3})\Big).
\end{equation}
In our particular situation with two vertices, it's quite
simple. For figure \ref{fig:img1}, it's
\begin{equation}%\label{eq:}
\partial_{0}:k^{1}e_{1}+\ldots+k^{3}e_{3}\mapsto
(k^{1}+k^{2}+k^{3})v_{2} + (-k^{1}-k^{2}-k^{3})v_{1}
\end{equation}
which isn't too surprising. Now, we just defined a cycle as a
chain where the coefficients of $v_{1}$ and $v_{2}$ in this
equation are zero. This is the nullspace, or the ``kernel'', of
the linear operator $\partial_{0}$. So we have precisely now the
rigorous algebraic condition for a cycle: \emph{a cycle is a
  chain that lives in the kernel of the linear operator
  $\partial_{0}$.}

We can check to see that $e_{1}-e_{2}$ and $e_{1}-e_{3}$ form a
basis of the nullspace. These paths enclose a ``hole'' of the
graph, in a more homotopic sense (if there were no such ``hole'',
we could homotopically deform the path however we want -- since
there ``is'' a ``hole'', we cannot do this!).

\marginnote{\vspace{-4pc}\begin{center}\includegraphics{img/img.3}\\ \refstepcounter{figure} {\sc Figure~\thefigure}\end{center}\label{fig:img3}}

We can revisit our diagram from figure \ref{fig:img2} and take it
up a notch. We can consider the region bounded by two edges. This
is colored green and labeled $A$ in figure \ref{fig:img3}. This
region bounded by a cycle is called a 2-cell, or a face, or a plaquette.
It has as boundary the cycle $e_{1}-e_{2}$. If we pretend for a
little bit that we contract these two edges $e_{1}$ and $e_{2}$
closer and closer together, making the region $A$ smaller and
smaller until it no longer exists, we are left with two cycles:
$e_{1}-e_{3}$ and $e_{3}-e_{4}$. Why do we ``deform'' our beloved
green region so? Well, it's a homotopic deformation, so it's
kosher topologically speaking. It simply identifies the cycle
$e_{1}-e_{2}$ as ``the identity'' (i.e. ZERO! since we have squeezed
$e_{1}$ into $e_{2}$, making the two ``the same'').

What's going on algebraically? Well, we have three ``basis
cycles'' for the kernel of $\partial_{0}$: $e_{1}-e_{2}$,
$e_{1}-e_{3}$ and $e_{3}-e_{4}$. Since we have ``squeezed''
things together with $A$, it basically turns $e_{1}-e_{2}$
``into'' the identity element (which is 0 for Abelian groups). We
can introduce the free group generated by the 2-cells (in our
case, we only have one --- $A$) denote this free group by
$C_{2}$. We can analogously construct an operator
\begin{equation}%\label{eq:}
\begin{split}
\partial_{1}:~&C_{2}\to{C_{1}}\\
              &A~\mapsto{e_{1}-e_{2}}
\end{split}
\end{equation}
which takes the 2-cell $A$ to its boundary. We can consider now
the so-called ``quotient group''
$\ker(\partial_{0})/\im(\partial_{1})$ which basically identifies
the elements of $\im(\partial_{1})$ that live in
$\ker(\partial_{0})$ to be zero (the identity element). This is
analogous to the modulo operator in most programming
languages. As a consequence, we see that
\begin{equation}%\label{eq:}
\begin{split}
(e_{1}-e_{2})+(e_{2}-e_{3}) &= e_{1}-e_{3}\\
 &\equiv e_{2}-e_{3} \mod {e_{1}-e_{2}}
\end{split}
\end{equation}
This should not be too surprising given the intuition that we've
basically taken $A$ ``vanishes'' which identifies $e_{1}$ as
``the same'' as $e_{2}$. \marginpar{Homology Group
  $H_{1}(X_{1})$}This quotient group, however, is given a very
special name and is of very special importance: it's called the
\textbf{``Homology Group''} and denoted by $H_{1}(X_{1})$. The
$X_{p}$ refers to $p$-cells; we have 0-cells be vertices, 1-cells
be edges, and 2-cells be ``faces''. (The reader can probably
guess that the ultimate aim of this introductory approach is to
point out how difficult it is to consider generalizing this to
$p$-cells, working with some abstract and hard to compute
$\partial_{p-1}$ operator, and so we'll introduce some ingenius
way around these difficulties.)


