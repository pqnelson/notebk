%%
%% entropicForce.tex
%% 
%% Made by alex
%% Login   <alex@tomato>
%% 
%% Started on  Wed Feb 15 15:36:34 2012 alex
%% Last update Wed Feb 15 15:36:34 2012 alex
%%
\begin{defn}
An \define{Entropic Force} is an effective macroscopic force that
originates in a system with many degrees of freedom by the
statistical tendency to increase its entropy.
\end{defn}

We express the force equation in terms of entropy differences.

\begin{prop}
An entropic force is independent of the details of the
microscopic dynamics.
\end{prop}

\begin{prop}
No fundamental field is associated with an entropic force.
\end{prop}

\begin{ex}
The elasticity of a polymer molocule is an entropic force. A
single polymer molecule can be modeled as many monomers of fixed
length, where each monomer can rotate freely around the points of
attachment and direct itself in any spatial direction. Each of
these configurations has the same energy. When the polymer
molecule is immersed into a heat bath, it puts itself into a
randomly coiled configuration (as these are entropically
favored). There more configurations when the molecule is short,
compared to when it is stretched to an extended
configuration. The tendency to return to a maximal entropy states
translates into a macroscopic force, which is the elastic force.

What does this mean in plain English? Well, a rubber band has a
tendency to return to its relaxed position. But why? Some might
be tempted to say its because the stretched rubber band has more
\emph{energy} than an unstretched one. This works for a
\emph{metal spring}. What about a rubber one? The microscopic
description of rubber suggests it is due to differences in
\emph{entropy}. 
\end{ex}



\begin{defn}
For a given system, the \define{Entropy} equals
\begin{equation}
S(E,x)=k_{B}\log\Omega(E,x)
\end{equation}
where $k_{B}$ is Boltzmann's constant and $\Omega(E,x)$ denotes
the volume of the configuration space for the entire system.
\end{defn}
\begin{rmk}
Note that there are other notions of entropy. One particularly
interesting notion is, if we fix some programming language (or
machine language, or Turing machine, or \dots)
\define{Algorithmic Entropy}, or ``dually'' the
\define{Algorithmic Information} of a number is the length of the
shortest program having that number as output.  

We may also have the algorithmic information of a \emph{finite
bit string}, which is the length of the shortest program that has
that bit string as output. 
\end{rmk}
