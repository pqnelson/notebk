\section{Linear Transformations}

\M
We have, so far, introduced a new shiny toy, a new mathematical gadget
called a ``vector space''. So far, we have discussed quite a few aspects
about them, except for one very important thing: how do we ``map'' one
vector space into another?

\begin{definition}
Let $V$, $W$ be real vector spaces.
A \define{Linear Transformation} is a function $L\colon V\to W$ such
that for each $\vec{u}$, $\vec{v}\in V$ and for any $c_{1},c_{2}\in\RR$,
we have it map linear combinations in $V$ to linear combinations in $W$:
\begin{equation}
L(c_{1}\vec{u} + c_{2}\vec{v}) = c_{1}L(\vec{u}) + c_{2}L(\vec{v}).
\end{equation}
If further $V=W$, then we call $L$ a \define{Linear Operator}.
We may also just refer to the function $f$ as \emph{Linear}.
\end{definition}

\begin{remark}
  Some authors give two conditions for $L$ being a linear transformation:
  \begin{enumerate}
  \item homogeneity: $L(c\vec{v})=cL(\vec{v})$ for every $c\in\RR$ and
    $\vec{v}\in V$
  \item additive: $L(\vec{u}+\vec{v})=L(\vec{u})+L(\vec{v})$ for every
    $\vec{u}$, $\vec{v}\in V$.
  \end{enumerate}
  Can you prove these two versions are secretly the same?
\end{remark}

\begin{example}
Let $V$ be a real vector space. The identity function $\id\colon V\to V$,
defined by $\id(\vec{v})=\vec{v}$ for all $\vec{v}\in V$, is a linear
operator on $V$.

\begin{proof}
Let $\vec{u}$, $\vec{v}\in V$ be arbitrary. Let $c_{1},c_{2}\in\RR$ be arbitrary.
We want to prove
\begin{equation}
\id(c_{1}\vec{u} + c_{2}\vec{v}) = c_{1}\id(\vec{u}) + c_{2}\id(\vec{v}).
\end{equation}
We see that the left-hand side expands to
\begin{equation}
\id(c_{1}\vec{u} + c_{2}\vec{v}) = c_{1}\vec{u} + c_{2}\vec{v},
\end{equation}
and the right-hand side expands to
\begin{equation}
c_{1}\id(\vec{u}) + c_{2}\id(\vec{v}) = c_{1}\vec{u} + c_{2}\vec{v}.
\end{equation}
Then we invoke the well-known fact that,
\begin{equation}
c_{1}\vec{u} + c_{2}\vec{v} = c_{1}\vec{u} + c_{2}\vec{v}
\end{equation}
to conclude we must have
\begin{equation}
\id(c_{1}\vec{u} + c_{2}\vec{v}) = c_{1}\id(\vec{u}) + c_{2}\id(\vec{v}).
\end{equation}
as desired.
\end{proof}
\end{example}

\begin{non-example}[Shifting is not a linear transformation]
Let $V$ be a vector space, let $\vec{v}_{0}\in V$ be a nonzero vector.
Define $f\colon V\to V$ by
\begin{equation}
f(\vec{v})=\vec{v}+\vec{v}_{0}.
\end{equation}
This is not a linear transformation. Why not? Well, suppose we have
$\vec{u}\in V$ and $\vec{v}\in V$, then
\begin{calculation}
  f(\vec{u} + \vec{v})
\step{unfold definition of $f$}
  (\vec{u} + \vec{v})+\vec{v}_{0}
\end{calculation}
However,
\begin{calculation}
  f(\vec{u}) + f(\vec{v})
\step{unfold definition of $f$}
  (\vec{u} +\vec{v}_{0}) + (\vec{v}+\vec{v}_{0})
\end{calculation}
We see that, in general,
\begin{equation}
(\vec{u} +\vec{v}_{0}) + (\vec{v}+\vec{v}_{0})\neq(\vec{u} + \vec{v})+\vec{v}_{0}.
\end{equation}
Hence
\begin{equation}
f(\vec{u} + \vec{v})\neq f(\vec{u}) + f(\vec{v}).
\end{equation}
Or to summarize it in a single equation
\begin{equation}
\begin{array}{ccc}
f(\vec{u} + \vec{v}) &\stackrel{???}{=}& f(\vec{u}) + f(\vec{v})\\
\verteq & & \verteq\\
(\vec{u} + \vec{v})+\vec{v}_{0} & \neq & (\vec{u} +\vec{v}_{0}) + (\vec{v}+\vec{v}_{0})
\end{array}
\end{equation}
which means $f$ cannot be linear.
\end{non-example}

\begin{example}
Let $c\in\RR$ be a nonzero constant $c\neq0$, let $V$ be a real vector space.
Let $f\colon V\to V$ be defined by
\begin{equation}
f(\vec{v}) = c\vec{v}.
\end{equation}
Then $f$ is a linear operator.

\begin{proof}
Let $\vec{u},\vec{v}\in V$ be arbitrary, let $c_{1},c_{2}\in\RR$ be arbitrary.
We will show $f(c_{1}\vec{u}+c_{2}\vec{v})=c_{1}f(\vec{u})+c_{2}f(\vec{v})$.
We begin by expanding
\begin{calculation}
  f(c_{1}\vec{u}+c_{2}\vec{v})
\step{unfold definition of $f$}
  c(c_{1}\vec{u}+c_{2}\vec{v})
\step{distributivity of scalar multiplication}
  cc_{1}\vec{u}+cc_{2}\vec{v}
\step{associativity of scalar multiplication}
  c(c_{1}\vec{u})+c(c_{2}\vec{v})
\step{folding the definition of $f$}
  f(c_{1}\vec{u})+f(c_{2}\vec{v})
\end{calculation}
This proves $f$ is linear.
\end{proof}
\end{example}

\begin{example}
  Let $\vec{u}\in V$ be a nonzero vector. Then projection along
  $\vec{u}$,
  \begin{subequations}
  \begin{equation}
\operatorname{Proj}_{\vec{u}}\colon V\to V
  \end{equation}
  sends
\begin{equation}
\vec{v}\mapsto\frac{\vec{u}\cdot\vec{v}}{\vec{u}\cdot\vec{u}}\vec{u}.
\end{equation}
  \end{subequations}
This is a linear operator.
\end{example}

\begin{example}
Let $\widehat{\vec{x}}\in V$ be a unit vector.
We define the \define{Householder transformation}
\begin{subequations}
  \begin{align}
    H_{\vec{x}}\colon &V\to V
    \intertext{by}
    &\vec{v}\mapsto \vec{v} - \widehat{\vec{x}}(\widehat{\vec{x}}\cdot\vec{v}).
  \end{align}
\end{subequations}
This describes a reflection about the plane whose normal vector is $\widehat{\vec{x}}$.
We see this is the sum of two linear operators, the identity matrix, and
the projection in the direction of $-\vec{x}$. The sum of linear
operators is itself a linear operator.
\end{example}

\N{Other examples}
There are a few other examples which are useful, in particular, if
$V$ is a vector space and $U\subset V$ is a subspace, then the inclusion
map
\begin{equation}
\iota\colon U\to V
\end{equation}
defined by $\iota(\vec{u})=\vec{u}$ for every $\vec{u}\in U$, this is a
linear transformation. We could also go the other way around: we could
construct a mapping
\begin{equation}
p\colon V\to U
\end{equation}
which projects to $U$. It's a little trickier \emph{at this point} to
prove it, though.

\begin{definition}
  When $L\colon V\to W$ is a linear transformation, we call:
\begin{itemize}
\item the inputs $V$ the \define{Domain} of $L$,
\item the possible outputs $W$ the \define{Codomain} of $L$,
\item the \define{Image} of $L$ is the subset $L(V) = \{L(\vec{v})\in W\mid\vec{v}\in V\}$
of vectors in $W$ which are the output of $L$.
\end{itemize}
Note: the codomain consists of the \emph{possible} outputs, the image
consists of the \emph{actual} outputs.
\end{definition}

\begin{definition}
If $L\colon V\to W$ is a linear transformation, we can define its
\define{Inverse} to be a linear transformation $L^{-1}\colon W\to V$
such that $L^{-1}\circ L=\id_{V}$ and $L\circ L^{-1}=\id_{W}$.
\end{definition}

\M
A lot of the terminology for matrices carries over to linear
transformations. For example, if $L\colon V\to V$ is a linear operator,
we call it \define{Similar} to a linear operator $M\colon V\to V$ if
there exists an invertible linear operator $P\colon V\to V$ such that
$L = P^{-1}\circ M \circ P$.
Compare this to similar matrices, $\mat{A} \sim\mat{B}$ if there exists
an invertible $\mat{P}$ such that $\mat{A}=\mat{P}^{-1}\mat{B}\mat{P}$.
We may abuse notation and write $L\sim M$ when two linear operators are
similar.

\subsection{Matrices as ``Coordinates'' of Linear Transformations}

\M
We saw how basis vectors, for a finite-dimensional vector space $V$,
allows us to introduce coordinates for vectors (in
Definition~\ref{defn:basis:coordinates}).
This was great because it allowed us to use some of our matrix machinery
we built in Part~\ref{part:matrices} when computing vector operations.
But there is more: basis vectors let us express a linear transformation
as a matrix, and ``the linear transformation acting on a vector'' as
mere matrix multiplication.

\begin{definition}
Let $V$, $W$ be [finite-dimensional] vector spaces with bases
$B_{V}=(\vec{e}_{1},\dots,\vec{e}_{m})$ and $B_{W}=(\vec{f}_{1},\dots,\vec{f}_{n})$
respectively. Let $L\colon V\to W$ be a linear transformation.
Then the \define{Matrix for $L$ relative to $B_{V}$ and $B_{W}$}
is the matrix $\mat{M}$ whose $j^{\text{th}}$ column is the coordinate vector
for $L(\vec{e}_{j})$ relative to $B_{W}$.
Then, for any $\vec{v}\in V$, we may compute $L(\vec{v})$ using
$\mat{M}$ multiplied by the coordinate vector of $\vec{v}$ relative to
basis $\vec{e}_{j}$.
\end{definition}

\begin{remark}
Just because we \emph{defined} something doesn't mean it works as
intended, or even exists. We need to \emph{prove it} in a theorem.
\end{remark}

\begin{theorem}[This stuff works]
Let $L\colon V\to W$ be a linear transformation from an $m$-dimensional
real vector space $V$ to an $n$-dimensional real vector space $W$.
Let $B_{V}=(\vec{e}_{1}, \dots, \vec{e}_{m})$ be a basis for $V$; let
$B_{W}=(\vec{f}_{1}, \dots, \vec{f}_{n})$ be a basis for $W$.

Then the $n\times m$ matrix $\mat{M}$ --- whose $j^{\text{th}}$ column is
the coordinate vector $[L(\vec{e}_{j})]_{B_{W}}$ of $L(\vec{e}_{j})$
relative to the basis $B_{W}$ --- is associated with $L$ and has the
following property: For any $\vec{v}\in V$, we have
\begin{equation}
[L(\vec{v})]_{B_{W}} = \mat{M}[\vec{v}]_{B_{V}}
\end{equation}
where $[\vec{v}]_{B_{V}}$ is the $m\times1$ coordinate vector of
$\vec{v}$ relative to $B_{V}$, and $[L(\vec{v})]_{B_{W}}$ is the $n\times1$
coordinate vector of $L(\vec{v})$ relative to $B_{W}$. Moreover,
$\mat{M}$ is the only matrix with this property.
\end{theorem}

\begin{proof}[Proof sketch]
The idea is to to construct such a matrix $\mat{M}$ from $L\colon V\to W$
and bases $B_{V}=(\vec{e}_{1},\dots,\vec{e}_{m})$ and
$B_{W}=(\vec{f}_{1},\dots,\vec{f}_{n})$, as follows:
\begin{itemize}
\item[Step 1.] Compute $L(\vec{e}_{j})$ for $j=1,\dots,m$.
\item[Step 2.] Find the coordinate vector $[L(\vec{e}_{j})]_{B_{W}}$ for
  $L(\vec{e}_{j})$ relative to the basis $B_{W}$. This means expressing
  $L(\vec{e}_{j})$ as a linear combination of the $\vec{f}_{i}$.
\item[Step 3.] The matrix $\mat{M}$ of $L$ with respect to $B_{V}$ and
  $B_{W}$ is formed by choosing $[L(\vec{e}_{j})]_{B_{W}}$ as the
  $j^{\text{th}}$ column of $\mat{M}$.
\end{itemize}
In this way, any vector $\vec{v} = c_{1}\vec{e}_{1}+\dots+c_{m}\vec{e}_{m}$
is mapped to $L(\vec{v}) = c_{1}L(\vec{e}_{1}) + c_{2}L(\vec{e}_{2}) + \dots + c_{m}L(\vec{e}_{m})$
by definition of a linear transformation, and this coincides with matrix
multiplication $\mat{M}[\vec{v}]_{B_{V}}$.
\end{proof}

\M
What do we do if the bases for either vector space is not canonical or
orthogonal? The trick is to form an augmented matrix
\begin{equation}
[\vec{f}_{1}\quad\dots\quad f_{n}\mid L(\vec{e}_{1})\quad\dots\quad L(\vec{e}_{m})]
\end{equation}
then applying elementary row operations, we transform it to reduced row
echelon form, and then keep transforming it until it becomes
\begin{equation}
[\vec{f}_{1}\quad\dots\quad f_{n}\mid L(\vec{e}_{1})\quad\dots\quad L(\vec{e}_{m})]\sim[\mat{I}\mid\mat{M}].
\end{equation}
We then identify $\mat{M}$ as the matrix for $L$ relative to the basis
$B_{V}$ and $B_{W}$.

\subsection{Injective, Surjective, Bijective Linear Transformations}

\begin{definition}
  Let $L\colon V\to W$ be a linear transformation. We call $L$
  \begin{itemize}
  \item \define{Surjective} (or \emph{Onto}) if, to each and every $\vec{w}\in W$,
    there exists a $\vec{v}\in V$ such that $L(\vec{v})=\vec{w}$
  \item \define{Injective} (or, confusingly, \emph{into}) if for any
    $\vec{v}_{1},\vec{v}_{2}\in V$ such that
    $L(\vec{v}_{1})=L(\vec{v}_{2})$, then we have
    $\vec{v}_{1}=\vec{v}_{2}$ --- equivalently, if
    $\vec{v}_{1}\neq\vec{v}_{2}$, then $L(\vec{v}_{1})\neq L(\vec{v}_{2})$.
  \item \define{Bijective} if it is both surjective and injective.
  \end{itemize}
\end{definition}

\begin{remark}
These terms (injective, surjective, bijective) hold for \emph{any}
function on sets. That is to say, these are not ``linear algebra
specific terms''.
\end{remark}

\begin{example}[Surjective map]
Let $U\subset V$ be a subspace. The map $L\colon V\to U$, defined by for
any $\vec{u}\in U$, $L(\vec{u})=\vec{u}$, and for any $\vec{v}\in V$ but
$\vec{v}\notin U$ we have $L(\vec{v})=\vec{0}$. This is a surjective
linear transformation.

\begin{proof}
Surjectivity isn't hard, we're told every $\vec{u}\in U$ are mapped to
$L(\vec{u})=\vec{u}$. Linearity may be a bit more difficult. If we had
some basis $B_{U}$ of $U$ which extends to a basis of $V$, then we see
\begin{equation}
L(\sum_{j=1}^{m}c_{j}\vec{b}_{j} + \sum_{k=m+1}^{n}d_{k}\vec{f}_{k}) = \sum^{m}_{j=1}c_{j}\vec{b}_{j}
\end{equation}
where $\vec{b}_{j}\in B_{U}$ for each $j=1,\dots,m$ and
$\vec{f}_{k}\notin B_{U}$ are basis vectors for the rest of $V$ (which
would be mapped to zero), and $c_{j}\in\RR$ for $j=1,\dots,m$ and
$d_{k}\in\RR$ for $k=m+1,\dots,n$. This is indeed linear, by definition.
\end{proof}
\end{example}

\N{Surjective Linear Maps}
A surjective linear map is sometimes denoted with a two-headed arrow
$L\colon V\onto W$. We intuitively think of surjectivity as ``covering''
the entire codomain.

\begin{example}[Injective map]
Let $U\subset V$ be a subspace. Then $L\colon U\to V$, defined by $L(\vec{u})=\vec{u}$,
is an injective linear map.

\begin{proof}
Let $\vec{u}_{1},\vec{u}_{2}\in U$ be arbitrary. Assume $\vec{u}_{1}\neq\vec{u}_{2}$.
Then $L(\vec{u}_{1})=\vec{u}_{1}\neq\vec{u}_{2}=L(\vec{u}_{1})$, hence
$L(\vec{u}_{1})\neq L(\vec{u}_{2})$. Then by definition, $L$ is injective.
\end{proof}
\end{example}

\N{Injective maps embed}
An injective linear map is sometimes denoted with the arrow $L\colon V\into W$
with a hooked arrow. This intuitively corresponds to embedding $V$ into $W$.
That is to say, there exists a subspace of $W$ which is ``the same'' as
$V$.

\N{Isomorphisms}
A bijective linear transformation is also called a \define{Isomorphism}
of vector spaces. If $L\colon V\to W$ is a bijective linear
transformation, then we may write $V\cong W$ to indicate there exists a
bijective linear transformation between them.

What this means is that, when viewed as vector spaces, $V$ and $W$ are
``the same''. To every element $\vec{v}\in V$ there is a unique
$\vec{w}\in W$ such that $L(\vec{v})=\vec{w}$.

\N{General Picture}
We can combine these insights to get some sense of what these terms
mean. For example, a surjective linear map is one whose codomain is
isomorphic to a subspace of the domain. An injective linear map is one
whose image is isomorphic with the domain.

\subsection{Kernels and Images}

\begin{definition}
Let $L\colon V\to W$ be a linear transformation. We define the
\define{Kernel} of $L$ to be the collection of vectors mapped to the
zero vector of $W$: $\ker(L)=\{\vec{v}\in V\mid L(\vec{v})=\vec{0}_{W}\}$.
\end{definition}

\begin{remark}
We can also talk about the ``kernel'' of a matrix, or other mathematical
mappings when there is a notion of ``zero'' (or a ``zero-like quantity'').
\end{remark}

\begin{proposition}
If $L\colon V\to W$ is a linear transformation, then $\ker(L)$ is a
subspace of $V$.
\end{proposition}

\begin{proof}
Claim 1: for any $\vec{v}$, $\vec{w}\in\ker(L)$, we have $\vec{v}+\vec{w}\in\ker(L)$.

We can see this from $L(\vec{v}+\vec{w}) = L(\vec{v})+L(\vec{w})$ by
linearity, and then $L(\vec{v})+L(\vec{w}) = \vec{0}_{W}+\vec{0}_{W} = \vec{0}_{W}$.
Hence $\vec{v}+\vec{w}\in\ker(L)$.

Claim 2: for any $\vec{v}\in\ker(L)$ and $c\in\RR$, we have
$(c\vec{v})\in\ker(L)$.

We can see this from $L(c\vec{v})=cL(\vec{v})$ due to linearity, and
$cL(\vec{v})=c\vec{0}_{W}=\vec{0}_{W}$. Hence $c\vec{v}\in\ker(L)$.

Then by Theorem~\ref{thm:subspaces:subset-closed-under-linear-combos-is-a-subspace},
$\ker(L)$ is a subspace of $V$.
\end{proof}

\begin{proposition}
Let $L\colon V\to W$ be a linear transformation.
If $\vec{v}_{1},\vec{v}_{2}\in V$ are mapped to the same element
$L(\vec{v}_{1})=L(\vec{v}_{2})$, then their difference lives in the
kernel of $L$, $\vec{v}_{2}-\vec{v}_{1}\in\ker(L)$.
\end{proposition}

\begin{proof}
  Assume $L(\vec{v}_{1})=L(\vec{v}_{2})$. Then
  \begin{calculation}
    L(\vec{v}_{1})=L(\vec{v}_{2})
    \step[\equiv]{subtracting $L(\vec{v}_{1})$ from both sides}
    L(\vec{v}_{2})-L(\vec{v}_{1})=\vec{0}_{W}
    \step[\equiv]{linearity}
    L(\vec{v}_{2}-\vec{v}_{1})=\vec{0}_{W}
  \end{calculation}
  hence $\vec{v}_{2}-\vec{v}_{1}\in\ker(L)$ as desired.
\end{proof}

\begin{theorem}
  Let $L\colon V\to W$ be a linear transformation.
  Then $L$ is an injective linear map if and only if $\ker(L)=0$ is the trivial subspace.
\end{theorem}
\begin{proof}
$(\implies)$ Assume $L$ is an injective map. For every $\vec{v}\in V$
  such that $\vec{v}\neq\vec{0}_{V}$ we have $L(\vec{v})\neq L(\vec{0}_{V})=\vec{0}_{W}$.
  Then $L(\vec{v})\neq\vec{0}_{W}$, which implies $\vec{v}\notin\ker(L)$
  when $\vec{v}\neq\vec{0}_{V}$. Hence $\ker(L)=\{\vec{0}_{V}\}$.

$(\impliedby)$ Assume $\ker(L)=0=\{\vec{0}_{V}\}$.
  Then for any $\vec{v}_{1},\vec{v}_{2}\in V$ such that
  $L(\vec{v}_{1})=L(\vec{v}_{2})$, we see $L(\vec{v}_{2})-L(\vec{v}_{1})=\vec{0}_{W}$.
  By linearity, we know $L(\vec{v}_{2})-L(\vec{v}_{1})=L(\vec{v}_{2}-\vec{v}_{1})$,
  hence $\vec{v}_{2}-\vec{v}_{1}\in\ker(L)$. But this implies $\vec{v}_{2}-\vec{v}_{1}=\vec{0}_{V}$,
  which means $\vec{v}_{2}=\vec{v}_{1}$. Hence $L$ is injective.
\end{proof}

\N{Dimensions of Kernel}
Let $L\colon V\to W$ be a linear transformation, $V$ a
finite-dimensional vector space. Suppose $\dim\ker(L)=k<\dim(V)$.
What does this mean? Well, we have a $k$-dimensional subspace of $V$
which ``collapses'' under application of $L$, in the sense that
$L(\ker(L))=0$ is the trivial subspace of $W$.

But if $n=\dim(V)$, then what happens to the other $n-k$ dimensions of
$V$ under $L$? Well, there are two possibilities:
\begin{enumerate}
\item some elements of $V\setminus\ker(L)$ would be mapped to
  $\vec{0}_{W}$, or
\item no element of $V\setminus\ker(L)$ could be mapped to $\vec{0}_{W}$.
\end{enumerate}
If some element $\vec{v}$ in $V\setminus\ker(L)$ [i.e. $\vec{v}\in V$
  but $\vec{v}\notin\ker(L)$] is mapped to $\vec{0}_{W}$, then
$L(\vec{v})=\vec{0}_{W}$ which by definition makes it
$\vec{v}\in\ker(L)$. This is impossible, so we are in the second
possibility: no element of $V\setminus\ker(L)$ could be mapped to
$\vec{0}_{W}$.

Could $V\setminus\ker(L)$ form a subspace of $V$?
Technically, no, because $\vec{0}_{V}\in\ker(L)$, so it would be
impossible for $\vec{0}_{V}\in V\setminus\ker(L)$. Alright, well, what
about the set $U=\{\vec{0}_{V}\}\cup(V\setminus\ker(L))$, could this
form a subspace of $V$?

Let us consider a basis for $\ker(L)$. We could do this by finding $k$
linearly independent vectors, then applying the Graham--Schmidt
algorithm (\S\ref{chunk:graham-schmidt}) to form a basis $B_{K}$ of
$\ker(L)$. We can consider the canonical basis for $V$, then apply the
Graham--Schmidt algorithm to extend $B_{K}$ to an orthonormal basis $B$ of
all of $V$. The elements $B_{U}=\{\vec{b}\in B\mid\vec{b}\notin B_{K}\}$
form a basis for $U=\Span(B_{U})$. Moreover, $L(V)=L(U)$.

We see that $\dim(U)=|B_{U}|$ is the number of basis elements which do
not belong to the kernel, and $\dim(\ker(L))$ is the number of the
remaining basis vectors. Consequently,
\begin{equation}
\dim(U) + \dim(\ker(L)) = \dim(V).
\end{equation}
We also see that $\dim(U)=\dim(L(V))$. This gives us the celebrated
result
\begin{equation}
\boxed{\dim(L(V)) + \dim(\ker(L)) = \dim(V).}
\end{equation}
