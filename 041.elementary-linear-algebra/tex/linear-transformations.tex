\section{Linear Transformations}

\M
We have, so far, introduced a new shiny toy, a new mathematical gadget
called a ``vector space''. So far, we have discussed quite a few aspects
about them, except for one very important thing: how do we ``map'' one
vector space into another?

\begin{definition}
Let $V$, $W$ be real vector spaces.
A \define{Linear Transformation} is a function $L\colon V\to W$ such
that for each $\vec{u}$, $\vec{v}\in V$ and for any $c_{1},c_{2}\in\RR$,
we have it map linear combinations in $V$ to linear combinations in $W$:
\begin{equation}
L(c_{1}\vec{u} + c_{2}\vec{v}) = c_{1}L(\vec{u}) + c_{2}L(\vec{v}).
\end{equation}
If further $V=W$, then we call $L$ a \define{Linear Operator}.
We may also just refer to the function $f$ as \emph{Linear}.
\end{definition}

\begin{remark}
  Some authors give two conditions for $L$ being a linear transformation:
  \begin{enumerate}
  \item homogeneity: $L(c\vec{v})=cL(\vec{v})$ for every $c\in\RR$ and
    $\vec{v}\in V$
  \item additive: $L(\vec{u}+\vec{v})=L(\vec{u})+L(\vec{v})$ for every
    $\vec{u}$, $\vec{v}\in V$.
  \end{enumerate}
  Can you prove these two versions are secretly the same?
\end{remark}

\begin{example}
Let $V$ be a real vector space. The identity function $\id\colon V\to V$,
defined by $\id(\vec{v})=\vec{v}$ for all $\vec{v}\in V$, is a linear
operator on $V$.

\begin{proof}
Let $\vec{u}$, $\vec{v}\in V$ be arbitrary. Let $c_{1},c_{2}\in\RR$ be arbitrary.
We want to prove
\begin{equation}
\id(c_{1}\vec{u} + c_{2}\vec{v}) = c_{1}\id(\vec{u}) + c_{2}\id(\vec{v}).
\end{equation}
We see that the left-hand side expands to
\begin{equation}
\id(c_{1}\vec{u} + c_{2}\vec{v}) = c_{1}\vec{u} + c_{2}\vec{v},
\end{equation}
and the right-hand side expands to
\begin{equation}
c_{1}\id(\vec{u}) + c_{2}\id(\vec{v}) = c_{1}\vec{u} + c_{2}\vec{v}.
\end{equation}
Then we invoke the well-known fact that,
\begin{equation}
c_{1}\vec{u} + c_{2}\vec{v} = c_{1}\vec{u} + c_{2}\vec{v}
\end{equation}
to conclude we must have
\begin{equation}
\id(c_{1}\vec{u} + c_{2}\vec{v}) = c_{1}\id(\vec{u}) + c_{2}\id(\vec{v}).
\end{equation}
as desired.
\end{proof}
\end{example}

\begin{non-example}[Shifting is not a linear transformation]
Let $V$ be a vector space, let $\vec{v}_{0}\in V$ be a nonzero vector.
Define $f\colon V\to V$ by
\begin{equation}
f(\vec{v})=\vec{v}+\vec{v}_{0}.
\end{equation}
This is not a linear transformation. Why not? Well, suppose we have
$\vec{u}\in V$ and $\vec{v}\in V$, then
\begin{calculation}
  f(\vec{u} + \vec{v})
\step{unfold definition of $f$}
  (\vec{u} + \vec{v})+\vec{v}_{0}
\end{calculation}
However,
\begin{calculation}
  f(\vec{u}) + f(\vec{v})
\step{unfold definition of $f$}
  (\vec{u} +\vec{v}_{0}) + (\vec{v}+\vec{v}_{0})
\end{calculation}
We see that, in general,
\begin{equation}
(\vec{u} +\vec{v}_{0}) + (\vec{v}+\vec{v}_{0})\neq(\vec{u} + \vec{v})+\vec{v}_{0}.
\end{equation}
Hence
\begin{equation}
f(\vec{u} + \vec{v})\neq f(\vec{u}) + f(\vec{v}).
\end{equation}
Or to summarize it in a single equation
\begin{equation}
\begin{array}{ccc}
f(\vec{u} + \vec{v}) &\stackrel{???}{=}& f(\vec{u}) + f(\vec{v})\\
\verteq & & \verteq\\
(\vec{u} + \vec{v})+\vec{v}_{0} & \neq & (\vec{u} +\vec{v}_{0}) + (\vec{v}+\vec{v}_{0})
\end{array}
\end{equation}
which means $f$ cannot be linear.
\end{non-example}

\begin{example}
Let $c\in\RR$ be a nonzero constant $c\neq0$, let $V$ be a real vector space.
Let $f\colon V\to V$ be defined by
\begin{equation}
f(\vec{v}) = c\vec{v}.
\end{equation}
Then $f$ is a linear operator.

\begin{proof}
Let $\vec{u},\vec{v}\in V$ be arbitrary, let $c_{1},c_{2}\in\RR$ be arbitrary.
We will show $f(c_{1}\vec{u}+c_{2}\vec{v})=c_{1}f(\vec{u})+c_{2}f(\vec{v})$.
We begin by expanding
\begin{calculation}
  f(c_{1}\vec{u}+c_{2}\vec{v})
\step{unfold definition of $f$}
  c(c_{1}\vec{u}+c_{2}\vec{v})
\step{distributivity of scalar multiplication}
  cc_{1}\vec{u}+cc_{2}\vec{v}
\step{associativity of scalar multiplication}
  c(c_{1}\vec{u})+c(c_{2}\vec{v})
\step{folding the definition of $f$}
  f(c_{1}\vec{u})+f(c_{2}\vec{v})
\end{calculation}
This proves $f$ is linear.
\end{proof}
\end{example}

\begin{example}
  Let $\vec{u}\in V$ be a nonzero vector. Then projection along
  $\vec{u}$,
  \begin{subequations}
  \begin{equation}
\operatorname{Proj}_{\vec{u}}\colon V\to V
  \end{equation}
  sends
\begin{equation}
\vec{v}\mapsto\frac{\vec{u}\cdot\vec{v}}{\vec{u}\cdot\vec{u}}\vec{u}.
\end{equation}
  \end{subequations}
This is a linear operator.
\end{example}

\begin{example}
Let $\widehat{\vec{x}}\in V$ be a unit vector.
We define the \define{Householder transformation}
\begin{subequations}
  \begin{align}
    H_{\vec{x}}\colon &V\to V
    \intertext{by}
    &\vec{v}\mapsto \vec{v} - \widehat{\vec{x}}(\widehat{\vec{x}}\cdot\vec{v}).
  \end{align}
\end{subequations}
This describes a reflection about the plane whose normal vector is $\widehat{\vec{x}}$.
We see this is the sum of two linear operators, the identity matrix, and
the projection in the direction of $-\vec{x}$. The sum of linear
operators is itself a linear operator.
\end{example}

\N{Other examples}
There are a few other examples which are useful, in particular, if
$V$ is a vector space and $U\subset V$ is a subspace, then the inclusion
map
\begin{equation}
\iota\colon U\to V
\end{equation}
defined by $\iota(\vec{u})=\vec{u}$ for every $\vec{u}\in U$, this is a
linear transformation. We could also go the other way around: we could
construct a mapping
\begin{equation}
p\colon V\to U
\end{equation}
which projects to $U$. It's a little trickier \emph{at this point} to
prove it, though.

\begin{definition}
  When $L\colon V\to W$ is a linear transformation, we call:
\begin{itemize}
\item the inputs $V$ the \define{Domain} of $L$,
\item the possible outputs $W$ the \define{Codomain} of $L$,
\item the \define{Image} of $L$ is the subset $L(V) = \{L(\vec{v})\in W\mid\vec{v}\in V\}$
of vectors in $W$ which are the output of $L$.
\end{itemize}
Note: the codomain consists of the \emph{possible} outputs, the image
consists of the \emph{actual} outputs.
\end{definition}

\begin{definition}
If $L\colon V\to W$ is a linear transformation, we can define its
\define{Inverse} to be a linear transformation $L^{-1}\colon W\to V$
such that $L^{-1}\circ L=\id_{V}$ and $L\circ L^{-1}=\id_{W}$.
\end{definition}

\M
A lot of the terminology for matrices carries over to linear
transformations. For example, if $L\colon V\to V$ is a linear operator,
we call it \define{Similar} to a linear operator $M\colon V\to V$ if
there exists an invertible linear operator $P\colon V\to V$ such that
$L = P^{-1}\circ M \circ P$.
Compare this to similar matrices, $\mat{A} \sim\mat{B}$ if there exists
an invertible $\mat{P}$ such that $\mat{A}=\mat{P}^{-1}\mat{B}\mat{P}$.
We may abuse notation and write $L\sim M$ when two linear operators are
similar.

\subsection{Matrices as ``Coordinates'' of Linear Transformations}

\M
We saw how basis vectors, for a finite-dimensional vector space $V$,
allows us to introduce coordinates for vectors (in
Definition~\ref{defn:basis:coordinates}).
This was great because it allowed us to use some of our matrix machinery
we built in Part~\ref{part:matrices} when computing vector operations.
But there is more: basis vectors let us express a linear transformation
as a matrix, and ``the linear transformation acting on a vector'' as
mere matrix multiplication.

\begin{definition}
Let $V$, $W$ be [finite-dimensional] vector spaces with bases
$B_{V}=(\vec{e}_{1},\dots,\vec{e}_{m})$ and $B_{W}=(\vec{f}_{1},\dots,\vec{f}_{n})$
respectively. Let $L\colon V\to W$ be a linear transformation.
Then the \define{Matrix for $L$ relative to $B_{V}$ and $B_{W}$}
is the matrix $\mat{M}$ whose $j^{\text{th}}$ column is the coordinate vector
for $L(\vec{e}_{j})$ relative to $B_{W}$.
Then, for any $\vec{v}\in V$, we may compute $L(\vec{v})$ using
$\mat{M}$ multiplied by the coordinate vector of $\vec{v}$ relative to
basis $\vec{e}_{j}$.
\end{definition}

\begin{remark}
Just because we \emph{defined} something doesn't mean it works as
intended, or even exists. We need to \emph{prove it} in a theorem.
\end{remark}

\begin{theorem}[This stuff works]
Let $L\colon V\to W$ be a linear transformation from an $m$-dimensional
real vector space $V$ to an $n$-dimensional real vector space $W$.
Let $B_{V}=(\vec{e}_{1}, \dots, \vec{e}_{m})$ be a basis for $V$; let
$B_{W}=(\vec{f}_{1}, \dots, \vec{f}_{n})$ be a basis for $W$.

Then the $n\times m$ matrix $\mat{M}$ --- whose $j^{\text{th}}$ column is
the coordinate vector $[L(\vec{e}_{j})]_{B_{W}}$ of $L(\vec{e}_{j})$
relative to the basis $B_{W}$ --- is associated with $L$ and has the
following property: For any $\vec{v}\in V$, we have
\begin{equation}
[L(\vec{v})]_{B_{W}} = \mat{M}[\vec{v}]_{B_{V}}
\end{equation}
where $[\vec{v}]_{B_{V}}$ is the $m\times1$ coordinate vector of
$\vec{v}$ relative to $B_{V}$, and $[L(\vec{v})]_{B_{W}}$ is the $n\times1$
coordinate vector of $L(\vec{v})$ relative to $B_{W}$. Moreover,
$\mat{M}$ is the only matrix with this property.
\end{theorem}

\begin{proof}[Proof sketch]
The idea is to to construct such a matrix $\mat{M}$ from $L\colon V\to W$
and bases $B_{V}=(\vec{e}_{1},\dots,\vec{e}_{m})$ and
$B_{W}=(\vec{f}_{1},\dots,\vec{f}_{n})$, as follows:
\begin{itemize}
\item[Step 1.] Compute $L(\vec{e}_{j})$ for $j=1,\dots,m$.
\item[Step 2.] Find the coordinate vector $[L(\vec{e}_{j})]_{B_{W}}$ for
  $L(\vec{e}_{j})$ relative to the basis $B_{W}$. This means expressing
  $L(\vec{e}_{j})$ as a linear combination of the $\vec{f}_{i}$.
\item[Step 3.] The matrix $\mat{M}$ of $L$ with respect to $B_{V}$ and
  $B_{W}$ is formed by choosing $[L(\vec{e}_{j})]_{B_{W}}$ as the
  $j^{\text{th}}$ column of $\mat{M}$.
\end{itemize}
In this way, any vector $\vec{v} = c_{1}\vec{e}_{1}+\dots+c_{m}\vec{e}_{m}$
is mapped to $L(\vec{v}) = c_{1}L(\vec{e}_{1}) + c_{2}L(\vec{e}_{2}) + \dots + c_{m}L(\vec{e}_{m})$
by definition of a linear transformation, and this coincides with matrix
multiplication $\mat{M}[\vec{v}]_{B_{V}}$.
\end{proof}

\M
What do we do if the bases for either vector space is not canonical or
orthogonal? The trick is to form an augmented matrix
\begin{equation}
[\vec{f}_{1}\quad\dots\quad f_{n}\mid L(\vec{e}_{1})\quad\dots\quad L(\vec{e}_{m})]
\end{equation}
then applying elementary row operations, we transform it to reduced row
echelon form, and then keep transforming it until it becomes
\begin{equation}
[\vec{f}_{1}\quad\dots\quad f_{n}\mid L(\vec{e}_{1})\quad\dots\quad L(\vec{e}_{m})]\sim[\mat{I}\mid\mat{M}].
\end{equation}
We then identify $\mat{M}$ as the matrix for $L$ relative to the basis
$B_{V}$ and $B_{W}$.

\subsection{Injective, Surjective, Bijective Linear Transformations}

\begin{definition}
  Let $L\colon V\to W$ be a linear transformation. We call $L$
  \begin{itemize}
  \item \define{Surjective} (or \emph{Onto}) if, to each and every $\vec{w}\in W$,
    there exists a $\vec{v}\in V$ such that $L(\vec{v})=\vec{w}$
  \item \define{Injective} (or, confusingly, \emph{into}) if for any
    $\vec{v}_{1},\vec{v}_{2}\in V$ such that
    $L(\vec{v}_{1})=L(\vec{v}_{2})$, then we have
    $\vec{v}_{1}=\vec{v}_{2}$ --- equivalently, if
    $\vec{v}_{1}\neq\vec{v}_{2}$, then $L(\vec{v}_{1})\neq L(\vec{v}_{2})$.
  \item \define{Bijective} if it is both surjective and injective.
  \end{itemize}
\end{definition}

\begin{remark}
These terms (injective, surjective, bijective) hold for \emph{any}
function on sets. That is to say, these are not ``linear algebra
specific terms''.
\end{remark}

\begin{example}[Surjective map]
Let $U\subset V$ be a subspace. The map $L\colon V\to U$, defined by for
any $\vec{u}\in U$, $L(\vec{u})=\vec{u}$, and for any $\vec{v}\in V$ but
$\vec{v}\notin U$ we have $L(\vec{v})=\vec{0}$. This is a surjective
linear transformation.

\begin{proof}
Surjectivity isn't hard, we're told every $\vec{u}\in U$ are mapped to
$L(\vec{u})=\vec{u}$. Linearity may be a bit more difficult. If we had
some basis $B_{U}$ of $U$ which extends to a basis of $V$, then we see
\begin{equation}
L(\sum_{j=1}^{m}c_{j}\vec{b}_{j} + \sum_{k=m+1}^{n}d_{k}\vec{f}_{k}) = \sum^{m}_{j=1}c_{j}\vec{b}_{j}
\end{equation}
where $\vec{b}_{j}\in B_{U}$ for each $j=1,\dots,m$ and
$\vec{f}_{k}\notin B_{U}$ are basis vectors for the rest of $V$ (which
would be mapped to zero), and $c_{j}\in\RR$ for $j=1,\dots,m$ and
$d_{k}\in\RR$ for $k=m+1,\dots,n$. This is indeed linear, by definition.
\end{proof}
\end{example}

\N{Surjective Linear Maps}
A surjective linear map is sometimes denoted with a two-headed arrow
$L\colon V\onto W$. We intuitively think of surjectivity as ``covering''
the entire codomain.

\begin{example}[Injective map]
Let $U\subset V$ be a subspace. Then $L\colon U\to V$, defined by $L(\vec{u})=\vec{u}$,
is an injective linear map.

\begin{proof}
Let $\vec{u}_{1},\vec{u}_{2}\in U$ be arbitrary. Assume $\vec{u}_{1}\neq\vec{u}_{2}$.
Then $L(\vec{u}_{1})=\vec{u}_{1}\neq\vec{u}_{2}=L(\vec{u}_{1})$, hence
$L(\vec{u}_{1})\neq L(\vec{u}_{2})$. Then by definition, $L$ is injective.
\end{proof}
\end{example}

\N{Injective maps embed}
An injective linear map is sometimes denoted with the arrow $L\colon V\into W$
with a hooked arrow. This intuitively corresponds to embedding $V$ into $W$.
That is to say, there exists a subspace of $W$ which is ``the same'' as
$V$.

\N{Isomorphisms}
A bijective linear transformation is also called a \define{Isomorphism}
of vector spaces. If $L\colon V\to W$ is a bijective linear
transformation, then we may write $V\cong W$ to indicate there exists a
bijective linear transformation between them.

What this means is that, when viewed as vector spaces, $V$ and $W$ are
``the same''. To every element $\vec{v}\in V$ there is a unique
$\vec{w}\in W$ such that $L(\vec{v})=\vec{w}$.

\begin{definition}
Let $U$, $V$, $W$ be real vector spaces, let $L\colon U\to V$ and
$M\colon V\to W$ be linear transformations. We define the
\define{Composition} of $L$ followed by $M$ as a linear transformation
denoted $M\circ L\colon U\to W$ (written from right to left) such that
for any $\vec{u}\in U$, $(M\circ L)(\vec{u}) = M\left(L(\vec{u})\right)$.
\end{definition}

\N{Properties of Composition}
These are properties of composition which are true:
\begin{enumerate}
\item Associativity: for any $f\colon W\to X$, $g\colon X\to Y$, and
  $h\colon Y\to Z$, we have $(h\circ g)\circ f = h\circ(g\circ f)$.
\item Right Identity law: if $f\colon X\to Y$ is any function, and $\id_{X}$ is
  the identity function on $X$ (so every $x\in X$ satisfies $\id_{X}(x)=x$),
  then $f\circ\id_{X}=f$.
\item Left Identity law: if $f\colon X\to Y$ is any function, and $\id_{Y}$ is
  the identity function on $Y$ (so every $y\in Y$ satisfies $\id_{X}(y)=y$),
  then $\id_{Y}\circ f=f$.
\end{enumerate}
This is true for all functions, not just linear transformations.

\begin{remark}
For the collection of linear operators acting on a vector space $V$, we
see we can think of composition as a sort of ``multiplication''
operator (which is associative and has a ``number one''-like quantity,
the identity map $\id_{V}$). Further, we have the sum of linear
operators $L_{1},L_{2}\colon V\to V$ defined as
$(L_{1}+L_{2})(\vec{v})=L_{1}(\vec{v})+L_{2}(\vec{v})$ for all
$\vec{v}\in V$. We may define scalar multiplication similarly, for any
$c\in\RR$, $(cL_{1})(\vec{v})=c\left(L_{1}(\vec{v})\right)$. This gives
us a vector space structure on the collection of linear operators on
$V$, \emph{and} an associative multiplication operator; together these
form a gadget called a \emph{Associative Algebra}.

Oftentimes we forget all about the vector space structure, and use the
composition of operators to form a \emph{Monoid}. If we encode the state
of a system as a vector in $V$, then linear operators can be used to
describe how the state changes. This is what happens in quantum mechanics.
We can use it for any dynamical system, where the evolution of state by
an infinitesimal amount of time is described by a linear operator. The
properties of the linear operator reflect physical properties of the
dynamical system.
\end{remark}

\begin{definition}
  Let $V$, $W$ be real vector spaces. Let $T\colon V\to W$.
  \begin{enumerate}
  \item We define the \define{Right Inverse} of $T$ to be the linear
transformation $R\colon W\to V$ such that for each $\vec{w}\in W$,
$(T\circ R)(\vec{w})=\vec{w}$.
\item We define the \define{Left Inverse} of $T$ to be the linear
  transfromation $L\colon W\to V$ such that for each $\vec{v}\in V$,
$(L\circ T)(\vec{v})=\vec{v}$.
\item We define the \define{Two-Sided Inverse} (or more frequently, just
  ``\emph{Inverse\/}'') of $T$ to be the linear transformation $N\colon W\to V$
  such that $N$ is both a left-inverse and right-inverse of $L$.
  \end{enumerate}
\end{definition}

\begin{proposition}[Uniqueness of Inverse]
If $L\colon V\to W$ is a linear transformation with a two-sided inverse
$M\colon V\to W$, then $M$ is unique.
\end{proposition}

\begin{proof}
Suppose $M_{1},M_{2}\colon V\to W$ are a pair of two-sided inverses for $L$.
Let $\id_{V}\colon V\to V$ be the identity mapping on $V$,
$\id_{W}\colon W\to W$ be the identity mapping on $W$. Then
\begin{calculation}\setdefaultrelation{\equiv}
  \id_{W} = \id_{W}
  \step{since $M_{1}$, $M_{2}$ are right-inverses of $L$}
  (L\circ M_{1}) = (L\circ M_{2})
  \step{compose on left by $M_{1}$}
  M_{1}\circ (L\circ M_{1}) = M_{1}\circ (L\circ M_{2})
  \step{associativity of composing functions}
  (M_{1}\circ L)\circ M_{1} = (M_{1}\circ L)\circ M_{2}
  \step{since $M_{1}$ is a left-inverse of $L$}
  \id_{V}\circ M_{1} = \id_{V}\circ M_{2}
  \step{defining property of identity function}
  M_{1} = M_{2}
\end{calculation}
Hence any two two-sided inverses of $L$ must be equal to each other.
\end{proof}

\N{Notation}
Since the two-sided inverse is unique, we denote it by $T^{-1}$ as we do
for numbers.

\begin{proposition}[Right inverses exist for surjective maps]
Let $V$ and $W$ be real vector spaces, $T\colon V\to W$ be a
linear transformation. Then
$T$ is surjective if and only if
there exists a right-inverse $R\colon W\to V$ for $T$ (that is, $T\circ R=\id_{W}$).
\end{proposition}

\begin{proof}
  $(\implies)$ Assume $T$ is surjective. We want to find a $R\colon W\to V$
  such that for each $\vec{w}\in W$, $T\left(R(\vec{w})\right)=\vec{w}$.
  We know for each $\vec{w}\in W$ there is at least one $\vec{v}$ such
  that $T(\vec{v})=\vec{w}$, by definition of surjectivity. We could
  take a basis $B_{W}=\{\vec{f}_{1},\dots,\vec{f}_{n}\}$ for $W$, then
  for each basis element $\vec{f}_{j}\in B_{W}$ take some corresponding
  $\vec{v}_{j}\in V$ such that $T(\vec{v}_{j})=\vec{f}_{j}$. We
  construct $R$ by $R(\vec{f}_{j})=\vec{v}_{j}$ and demanding linearity
  (which is possible because $T$ is lienar). Then
\begin{calculation}
  (T\circ R)(\vec{w})
  \step{expanding out using our basis $B_{W}$}
  (T\circ R)\left(\sum^{n}_{j=1}w_{j}\vec{f}_{j}\right)
  \step{unfolding composition of maps}
  T\left(R\left(\sum^{n}_{j=1}w_{j}\vec{f}_{j}\right)\right)
  \step{by $R$ being linear}
  T\left(\sum^{n}_{j=1}w_{j}R\left(\vec{f}_{j}\right)\right)
  \step{by construction of $R$ defined by $R(\vec{f}_{j})=\vec{v}_{j}$}
  T\left(\sum^{n}_{j=1}w_{j}\vec{v}_{j}\right)
  \step{by $T$ being linear}
  \sum^{n}_{j=1}w_{j}T\left(\vec{v}_{j}\right)
  \step{by construction of $\vec{v}_{j}$ such that $T(\vec{v}_{j})=\vec{f}_{j}$}
  \sum^{n}_{j=1}w_{j}\vec{f}_{j}
  \step{folding back $\vec{w}$ from its expansion relative to $B_{W}$}
  \vec{w}
\end{calculation}
Hence $R$ is a right-inverse, as desired.

  $(\impliedby)$ Assume $R\colon W\to V$ is a right-inverse for $T$,
  i.e.,
  $T\circ R=\id_{W}$.
  Then for each $\vec{w}\in W$, $T\left(R(\vec{w})\right)=\vec{w}$.
  But since $R(\vec{w})=\vec{v}$ for \emph{some} $\vec{v}\in V$, we see
  $T(\vec{v})=\vec{w}$. Since $\vec{w}$ was arbitrary, this means every
  $\vec{w}$ has at least one $\vec{v}\in V$ for which $T(\vec{v})=\vec{w}$
  (namely $\vec{v}=R(\vec{w})$). Hence $T$ is surjective, by definition.
\end{proof}

\begin{proposition}[Left inverses exist for injective maps]
Let $V$, $W$ be real vector spaces, $T\colon V\to W$ be a
linear transformation. Then $T$ is injective if and only if there exists a left inverse $L\colon W\to V$
for $T$, i.e., $L\circ T=\id_{V}$.
\end{proposition}

\begin{proof}
  $(\implies)$
Suppose $T$ is injective. Then for each $\vec{v}_{1},\vec{v}_{2}\in V$
such that $\vec{v}_{1}\neq\vec{v}_{2}$ we have $T(\vec{v}_{1})\neq T(\vec{v}_{2})$.
If we set $L(T(\vec{v}_{1}))=\vec{v}_{1}$ and for any $\vec{w}\notin T(V)$
is mapped to the zero vector $L(\vec{w})=\vec{0}_{V}$ (really this can
be any arbitrary vector, provided linearity is preserved), then $L$ is a
left-inverse function. 

$(\impliedby)$ Suppose there exists a left-inverse $L$ for $T$.
Then for any $\vec{v}_{1},\vec{v}_{2}\in V$ we see if
\begin{equation}
T(\vec{v}_{1})=T(\vec{v}_{2}),
\end{equation}
then we can apply $L$ to both sides
$L\left(T(\vec{v}_{1})\right)=L\left(T(\vec{v}_{2})\right)$ but by
definition of $L$ being a left inverse, this means
\begin{equation}
\vec{v}_{1}=\vec{v}_{2}.
\end{equation}
Hence $T$ is injective.
\end{proof}

\begin{example}
Let $V$ be a $n$-dimensional real vector space (for some $n\in\NN$).
Then there is a bijection $L\colon V\to\RR^{n}$.

\begin{proof}
Take $B=\{\vec{b}_{1},\dots,\vec{b}_{n}\}$ to be a basis for $V$.
We then define $L$ to map these basis vectors in $V$ to the
\hyperref[ex:basis:canonical-basis]{canonical basis} in $\RR^{n}$,
$L(\vec{b}_{j})=\vec{e}_{j}$. By linearity, we have linear combinations
mapped to linear combinations
\begin{equation}
L\left(\sum_{j=1}^{n}v_{j}\vec{b}_{j}\right)=\sum^{n}_{j=1}v_{j}L(\vec{b}_{j})=\sum^{n}_{j=1}v_{j}\vec{e}_{j}.
\end{equation}
We see this is a bijection because it's determined entirely by how it
acts on the basis vectors $B$, and each $\vec{b}_{j}\in B$ is mapped to
a distinct $\vec{e}_{j}$ [injectivity]. Every canonical basis vector
$\vec{e}_{j}$ is ``hit'' by exactly one $\vec{b}_{j}\in B$ [surjectivity].
We can define the inverse mapping $L^{-1}(\vec{e}_{j})=\vec{b}_{j}$ and
demand linearity.
\end{proof}
\end{example}

\N{General Picture}
We can combine these insights to get some sense of what these terms
mean. For example, a surjective linear map is one whose codomain is
isomorphic to a subspace of the domain. An injective linear map is one
whose image is isomorphic with the domain.

\subsection{Kernels and Images}

\begin{definition}\label{defn:linear-transformations:kernel}
Let $L\colon V\to W$ be a linear transformation. We define the
\define{Kernel} of $L$ to be the collection of vectors mapped to the
zero vector of $W$: $\ker(L)=\{\vec{v}\in V\mid L(\vec{v})=\vec{0}_{W}\}$.
\end{definition}

\begin{remark}
We can also talk about the ``kernel'' of a matrix, or other mathematical
mappings when there is a notion of ``zero'' (or a ``zero-like quantity'').
\end{remark}

\begin{proposition}
If $L\colon V\to W$ is a linear transformation, then $\ker(L)$ is a
subspace of $V$.
\end{proposition}

\begin{proof}
Claim 1: for any $\vec{v}$, $\vec{w}\in\ker(L)$, we have $\vec{v}+\vec{w}\in\ker(L)$.

We can see this from $L(\vec{v}+\vec{w}) = L(\vec{v})+L(\vec{w})$ by
linearity, and then $L(\vec{v})+L(\vec{w}) = \vec{0}_{W}+\vec{0}_{W} = \vec{0}_{W}$.
Hence $\vec{v}+\vec{w}\in\ker(L)$.

Claim 2: for any $\vec{v}\in\ker(L)$ and $c\in\RR$, we have
$(c\vec{v})\in\ker(L)$.

We can see this from $L(c\vec{v})=cL(\vec{v})$ due to linearity, and
$cL(\vec{v})=c\vec{0}_{W}=\vec{0}_{W}$. Hence $c\vec{v}\in\ker(L)$.

Then by Theorem~\ref{thm:subspaces:subset-closed-under-linear-combos-is-a-subspace},
$\ker(L)$ is a subspace of $V$.
\end{proof}

\begin{proposition}
Let $L\colon V\to W$ be a linear transformation.
If $\vec{v}_{1},\vec{v}_{2}\in V$ are mapped to the same element
$L(\vec{v}_{1})=L(\vec{v}_{2})$, then their difference lives in the
kernel of $L$, $\vec{v}_{2}-\vec{v}_{1}\in\ker(L)$.
\end{proposition}

\begin{proof}
  Assume $L(\vec{v}_{1})=L(\vec{v}_{2})$. Then
  \begin{calculation}
    L(\vec{v}_{1})=L(\vec{v}_{2})
    \step[\equiv]{subtracting $L(\vec{v}_{1})$ from both sides}
    L(\vec{v}_{2})-L(\vec{v}_{1})=\vec{0}_{W}
    \step[\equiv]{linearity}
    L(\vec{v}_{2}-\vec{v}_{1})=\vec{0}_{W}
  \end{calculation}
  hence $\vec{v}_{2}-\vec{v}_{1}\in\ker(L)$ as desired.
\end{proof}

\begin{theorem}
  Let $L\colon V\to W$ be a linear transformation.
  Then $L$ is an injective linear map if and only if $\ker(L)=0$ is the trivial subspace.
\end{theorem}
\begin{proof}
$(\implies)$ Assume $L$ is an injective map. For every $\vec{v}\in V$
  such that $\vec{v}\neq\vec{0}_{V}$ we have $L(\vec{v})\neq L(\vec{0}_{V})=\vec{0}_{W}$.
  Then $L(\vec{v})\neq\vec{0}_{W}$, which implies $\vec{v}\notin\ker(L)$
  when $\vec{v}\neq\vec{0}_{V}$. Hence $\ker(L)=\{\vec{0}_{V}\}$.

$(\impliedby)$ Assume $\ker(L)=0=\{\vec{0}_{V}\}$.
  Then for any $\vec{v}_{1},\vec{v}_{2}\in V$ such that
  $L(\vec{v}_{1})=L(\vec{v}_{2})$, we see $L(\vec{v}_{2})-L(\vec{v}_{1})=\vec{0}_{W}$.
  By linearity, we know $L(\vec{v}_{2})-L(\vec{v}_{1})=L(\vec{v}_{2}-\vec{v}_{1})$,
  hence $\vec{v}_{2}-\vec{v}_{1}\in\ker(L)$. But this implies $\vec{v}_{2}-\vec{v}_{1}=\vec{0}_{V}$,
  which means $\vec{v}_{2}=\vec{v}_{1}$. Hence $L$ is injective.
\end{proof}

\N{Dimensions of Kernel}
Let $L\colon V\to W$ be a linear transformation, $V$ a
finite-dimensional vector space. Suppose $\dim\ker(L)=k<\dim(V)$.
What does this mean? Well, we have a $k$-dimensional subspace of $V$
which ``collapses'' under application of $L$, in the sense that
$L(\ker(L))=0$ is the trivial subspace of $W$.

But if $n=\dim(V)$, then what happens to the other $n-k$ dimensions of
$V$ under $L$? Well, there are two possibilities:
\begin{enumerate}
\item some elements of $V\setminus\ker(L)$ would be mapped to
  $\vec{0}_{W}$, or
\item no element of $V\setminus\ker(L)$ could be mapped to $\vec{0}_{W}$.
\end{enumerate}
If some element $\vec{v}$ in $V\setminus\ker(L)$ [i.e. $\vec{v}\in V$
  but $\vec{v}\notin\ker(L)$] is mapped to $\vec{0}_{W}$, then
$L(\vec{v})=\vec{0}_{W}$ which by definition makes it
$\vec{v}\in\ker(L)$. This is impossible, so we are in the second
possibility: no element of $V\setminus\ker(L)$ could be mapped to
$\vec{0}_{W}$.

Could $V\setminus\ker(L)$ form a subspace of $V$?
Technically, no, because $\vec{0}_{V}\in\ker(L)$, so it would be
impossible for $\vec{0}_{V}\in V\setminus\ker(L)$. Alright, well, what
about the set $U=\{\vec{0}_{V}\}\cup(V\setminus\ker(L))$, could this
form a subspace of $V$?

Let us consider a basis for $\ker(L)$. We could do this by finding $k$
linearly independent vectors, then applying the Graham--Schmidt
algorithm (\S\ref{chunk:graham-schmidt}) to form a basis $B_{K}$ of
$\ker(L)$. We can consider the canonical basis for $V$, then apply the
Graham--Schmidt algorithm to extend $B_{K}$ to an orthonormal basis $B$ of
all of $V$. The elements $B_{U}=\{\vec{b}\in B\mid\vec{b}\notin B_{K}\}$
form a basis for $U=\Span(B_{U})$. Moreover, $L(V)=L(U)$.

We see that $\dim(U)=|B_{U}|$ is the number of basis elements which do
not belong to the kernel, and $\dim(\ker(L))$ is the number of the
remaining basis vectors. Consequently,
\begin{equation}
\dim(U) + \dim(\ker(L)) = \dim(V).
\end{equation}
We also see that $\dim(U)=\dim(L(V))$. This gives us the celebrated
result
\begin{equation}
\boxed{\dim(L(V)) + \dim(\ker(L)) = \dim(V).}
\end{equation}

\begin{definition}
  If $L\colon V\to W$ is a linear transformation, we call
  $\dim(\ker(L))$ its \define{Nullity} and $\dim(L(V))$ is
  \define{(Column) Rank}.
\end{definition}

\N{Importance of Linear Transformations}
Just as a concluding remark, I'd like to emphasize the importance of
linear transformations. So far, we have spent nearly 20 pages talking
about vector spaces before even thinking about linear
transformations. But this is for pedagogical reasons, to help the reader
get acquainted with the objects of linear algebra.

Secretly, all information concerning a vector space may be obtained from
linear transformations to, or from, the vector space.

This sounds preposterous and conspiratorial. But every possible subspace
is a kernel of some linear transformation (and the image of another).
Further, if we realize that a
linear transformation, when expressed relative to a basis, is a matrix,
then we see a lot of the machinery we constructed for matrices plays an
important role. There is one extraordinarily powerful \emph{coup} we can
introduced for matrices [eigenstuff], which --- when applied to linear
transformations --- demonstrates the power of these linear
transformations.
