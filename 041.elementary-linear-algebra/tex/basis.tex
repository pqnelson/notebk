\section{Linear Dependence and Bases}\label{section:basis}

\M
We want to address the question of whether this is a ``best'' spanning
set for a subspace, and we saw in some sense ``redundant elements''
should be avoided. If $\vec{s}_{1}\in S$ and $\vec{s}_{2}\in S$, then it
would be redundant to have $\vec{s}_{1}+\vec{s}_{2}\in S$. Let us try to
formalize this intuition of ``redundant combinations''.

\begin{definition}\label{defn:basis:linearly-dependent}
Let $V$ be a vector space, let $\vec{v}_{1}$, \dots, $v_{n}\in V$ be
nonzero vectors $\vec{v}_{j}\neq\vec{0}$ for $j=1,\dots,n$.
We call them \define{Linearly Dependent} if there are coefficients (not
all zero) $c_{1},\dots,c_{n}\in\RR$ such that
\begin{equation}
c_{1}\vec{v}_{1} + c_{2}\vec{v}_{2}+\cdots+c_{n}\vec{v}_{n}=\vec{0}.
\end{equation}
If the only solution for this is $c_{1}=c_{2}=\cdots=c_{n}=0$ for all
coefficients to be zero, then we call the vectors \define{Linearly Independent}.
\end{definition}

\begin{example}
  In $\RR^{2}$, consider the vectors
  \begin{equation}
\vec{v}_{1} = \begin{pmatrix} 1\\0 \end{pmatrix},
\vec{v}_{2} = \begin{pmatrix} 0\\1 \end{pmatrix},
\vec{v}_{3} = \begin{pmatrix} 1\\1 \end{pmatrix},
\vec{v}_{4} = \begin{pmatrix} 1\\-1 \end{pmatrix}.
  \end{equation}
  Any three or more vectors from this list are linearly dependent since
  $\vec{v}_{3}=\vec{v}_{1}+\vec{v}_{2}$ and
  $\vec{v}_{4}=\vec{v}_{1}-\vec{v}_{2}$. But
  any two vectors from this list are linearly independent.
\end{example}

\begin{theorem}[Criterion for Linear Dependence]
A set of nonzero vectors $\{\vec{v}_{1},\dots,\vec{v}_{n}\}$ is linearly
dependent if and only if at least one of the vectors $\vec{v}_{k}$ is
expressible as a linear combination of the others
\begin{equation}
\vec{v}_{k} = \sum^{n}_{\substack{j=1\\j\neq k}}c_{j}\vec{v}_{j} =c_{1}\vec{v}_{1} + \cdots + c_{k-1}\vec{v}_{k-1} + c_{k+1}\vec{v}_{k+1} +
  \cdots + c_{n}\vec{v}_{n},
\end{equation}
where not all coefficients $c_{j}\in\RR$ are zero.
\end{theorem}

\begin{proof}
  $(\implies)$ Assume the vectors $\vec{v}_{1}$, \dots, $\vec{v}_{n}$
  are linearly dependent. Then by Definition~\ref{defn:basis:linearly-dependent},
  there are coefficients $c_{1}$, \dots, $c_{n}$ (not all zero) such that
\begin{equation}
c_{1}\vec{v}_{1} + c_{2}\vec{v}_{2}+\cdots+c_{n}\vec{v}_{n}=\vec{0}.
\end{equation}
Let $k$ be the last index for which $c_{k}\neq0$ (so for indices $\ell$
such that  $k<\ell\leq n$, then $c_{\ell}=0$). Then we can subtract
$c_{k}\vec{v}_{k}$ from both sides to get
\begin{equation}
c_{1}\vec{v}_{1} + c_{2}\vec{v}_{2}+\cdots+c_{k}\vec{v}_{k}-c_{k}\vec{v}_{k}=\vec{0}-c_{k}\vec{v}_{k},
\end{equation}
and dividing both sides by $-c_{k}$ gives us $\vec{v}_{k}$ as a linear
combination of $\vec{v}_{1}$, \dots, $\vec{v}_{k-1}$. This concludes the
forward direction of the proof.

  $(\impliedby)$ Assume there exists a vector $\vec{v}_{k}$ such that we
  can write it as a linear combination of the remaining vectors
\begin{equation}
\begin{split}
  \vec{v}_{k} &= \sum^{n}_{\substack{j=1\\j\neq k}}c_{j}\vec{v}_{j}\\
  &=c_{1}\vec{v}_{1} + \cdots + c_{k-1}\vec{v}_{k-1} + c_{k+1}\vec{v}_{k+1} +
  \cdots + c_{n}\vec{v}_{n},
\end{split}
\end{equation}
where not all $c_{j}\in\RR$ are zero. Then subtracting $\vec{v}_{k}$
from both sides gives us
\begin{equation}
\vec{0} = c_{1}\vec{v}_{1} + \cdots + c_{k-1}\vec{v}_{k-1} - \vec{v}_{k} + c_{k+1}\vec{v}_{k+1} +
  \cdots + c_{n}\vec{v}_{n}.
\end{equation}
Then by Definition~\ref{defn:basis:linearly-dependent}, since not all
coefficients $c_{j}$ are zero, we have the vectors are linearly dependent.
\end{proof}

\begin{theorem}[Nonzero determinant iff columns are linearly independent]
Let $\{\vec{v}_{1},\dots,\vec{v}_{n}\}\subset\RR^{n}$ be a list of $n$
distinct $n$-vectors, and
\begin{equation}
\mat{M} = (\vec{v}_{1}|\dots|\vec{v}_{n})
\end{equation}
be a matrix whose columns are the given $n$ column vectors.
Then $\det(\mat{M})\neq0$ if and only if $\{\vec{v}_{1},\dots,\vec{v}_{n}\}$
are linearly independent.
\end{theorem}

\begin{proof}
$(\implies)$ Assume $\det(\mat{M})\neq0$. Then $\mat{M}$ is invertible
  (by Theorem~\ref{thm:determinant:singular-matrices-have-zero-det}).
Then $\mat{M}\vec{x}=\vec{0}$ has a unique solution (\S\ref{par:matrix-algebra:solving-systems-of-equations}), namely $\vec{x}=\vec{0}$.
This is equivalent to saying
\begin{equation}
x_{1}\vec{v}_{1} + x_{2}\vec{v}_{2} + \cdots + x_{n}\vec{v}_{n} = 0
\end{equation}
implies $x_{1}=x_{2}=\cdots=x_{n}=0$. But by Definition~\ref{defn:basis:linearly-dependent},
this is precisely the condition for $\vec{v}_{1}$, \dots, $\vec{v}_{n}$
being linearly independent.

$(\impliedby)$ Assume $\vec{v}_{1}$, \dots, $\vec{v}_{n}$ are linearly
independent. Then by Definition~\ref{defn:basis:linearly-dependent}, the
only solution to
\begin{equation}
c_{1}\vec{v}_{1} + \cdots + c_{n}\vec{v}_{n} = \vec{0}
\end{equation}
is $c_{1}=\cdots=c_{n}=0$. In matrix form, if $\vec{x}=(c_{1},\dots,c_{n})$
is a column $n$-vector, then
\begin{equation}
\mat{M}\vec{x} = \vec{0}
\end{equation}
has $\vec{x}=\vec{0}$ be its only solution. This is true if and only if
$\mat{M}$ is invertible. But $\mat{M}$ is invertible if and only if
$\det(\mat{M})\neq0$. And by our assumption, $\vec{x}=\vec{0}$ is the
only solution, hence the result.
\end{proof}

\begin{corollary}\label{cor:basis:invertible-matrix-iff-columns-are-linearly-independent}
An $n\times n$ matrix $\mat{M}$ is invertible if and only if its columns are
linearly independent vectors.
\end{corollary}
\begin{proof}
We know from the previous theorem $\mat{M}$ has nonzero determinant if
and only if its columns are linearly independent vectors. We know from Theorem~\ref{thm:determinant:singular-matrices-have-zero-det}
$\mat{M}$ has a nonzero determinant if and only if $\mat{M}$ is
invertible.
Therefore, we know $\mat{M}$ is invertible if and only if its columns
are linearly independent vectors.
\end{proof}

\begin{definition}
Let $V$ be a real vector space and $B$ a set of vectors from $V$ such
that
\begin{enumerate}
\item it spans $V$: $\Span(B)=V$
\item there is no $A\subset B$ such that $\Span(A)=V$.
\end{enumerate}
Then we call $B$ a \define{Basis} of $V$.
\end{definition}

\begin{remark}[Need to prove existence of basis]
We have just defined a word, ``basis'', but we have no guarantee that a
basis will exist. This must be proven. The proof is not enlightening,
and requires the axiom of choice (pick some nonzero vector, now pick
another which is linearly independent of the first, keep picking
linearly independent vectors --- how? By the axiom of choice, it's
always possible \emph{somehow}; then show an arbitrary vector may be
written as a linear combination of our collection of chosen vectors).
\end{remark}

\begin{example}
  In $\RR^{2}$, the vectors
  \begin{equation}
\vec{z} = \begin{pmatrix}1\\ 1
\end{pmatrix},\quad\mbox{and}\quad\bar{\vec{z}} = \begin{pmatrix}1\\ -1
\end{pmatrix}.
  \end{equation}
  Then $\{\vec{z}, \bar{\vec{z}}\}$ form a basis for $\RR^{2}$.
\end{example}
\begin{proof}
  We need to show
  \begin{enumerate}
  \item $\Span\{\vec{z}, \bar{\vec{z}}\}=\RR^{2}$
  \item there is no $A\subset\{\vec{z}, \bar{\vec{z}}\}$ such that $\Span(A)=\RR^{2}$.
  \end{enumerate}
  The first claim may be proven by picking any element
  $\vec{v}\in\RR^{2}$, then showing it may be written as a linear
  combination of $\vec{z}$ and $\bar{\vec{z}}$. We see, if
  \begin{equation}
\vec{v} = \begin{pmatrix}v_{1}\\v_{2}
\end{pmatrix},
  \end{equation}
  then
  \begin{calculation}
    \displaystyle\frac{v_{1}+v_{2}}{2}\vec{z} + \frac{v_{1}-v_{2}}{2}\bar{\vec{z}}
\step{unfolding the definition of $\vec{z}$, $\bar{\vec{z}}$}
    \displaystyle\frac{v_{1}+v_{2}}{2}\begin{pmatrix}1\\1
    \end{pmatrix}
    + \frac{v_{1}-v_{2}}{2}\begin{pmatrix}1\\-1
    \end{pmatrix}
\step{scalar multiplication}
    \displaystyle\frac{1}{2}\begin{pmatrix}v_{1}+v_{2}\\v_{1}+v_{2}
    \end{pmatrix}
    + \frac{1}{2}\begin{pmatrix}v_{1}-v_{2}\\-v_{1}+v_{2}
    \end{pmatrix}
\step{distributivity}
    \displaystyle\frac{1}{2}\left[\begin{pmatrix}v_{1}+v_{2}\\v_{1}+v_{2}
    \end{pmatrix}
    + \begin{pmatrix}v_{1}-v_{2}\\-v_{1}+v_{2}
    \end{pmatrix}\right]
\step{vector addition}
    \displaystyle\frac{1}{2}\begin{pmatrix}(v_{1}+v_{2})+(v_{1}-v_{2})\\(v_{1}+v_{2})+(-v_{1}+v_{2})
    \end{pmatrix}
\step{arithmetic}
    \displaystyle\frac{1}{2}\begin{pmatrix}2v_{1}\\2v_{2}
      \end{pmatrix}
\step{scalar multiplication}
    \displaystyle\begin{pmatrix}v_{1}\\v_{2}
      \end{pmatrix} = \vec{v}
  \end{calculation}
  as desired. Hence any element of $\RR^{2}$ may be written as a linear
  combination of $\vec{z}$ and $\bar{\vec{z}}$, hence $\Span(\{\vec{z},\bar{\vec{z}}\})=\RR^{2}$.

  As to the second claim, there is no $A\subset\{\vec{z},\bar{\vec{z}}\}$,
  suppose there were such an $A$. Then either $A=\{\vec{z}\}$ or
  $A=\{\bar{\vec{z}}\}$. Pick $\vec{v}\in\{\vec{z},\bar{\vec{z}}\}$ but
  $\vec{v}\notin A$. Then we claim $\vec{v}\notin\Span(A)$.

  It suffices to show $\vec{z}$ is not a multiple of $\bar{\vec{z}}$
  (which corresponds to $A=\{\bar{\vec{z}}\}$ --- in the other case, it
  boils down to the same proof). If $\vec{z}$ were a multiple of
  $\bar{\vec{z}}$, then there is a $c\in\RR$ nonzero such that
  \begin{equation}
c\begin{pmatrix}1\\1
\end{pmatrix} = \begin{pmatrix}1\\-1
\end{pmatrix}
  \end{equation}
  This is a system of 2 equations in 1 unknown:
  \begin{equation}
c=1,\quad\mbox{and}\quad c=-1.
  \end{equation}
  But this is impossible. So $\vec{z}$ cannot be a multiple of
  $\bar{\vec{z}}$, which means $\vec{z}\notin\Span(\{\bar{\vec{z}}\})$.
  The same reasoning shows $\bar{\vec{z}}$ is not a multiple of
  $\vec{z}$, which means $\bar{\vec{z}}\notin\Span(\{\vec{z}\})$.

  Hence there is no $A\subset\{\vec{z},\bar{\vec{z}}\}$ such that $\Span(A)=\RR^{2}$.
\end{proof}

\begin{example}\label{ex:basis:canonical-basis}
Let $\vec{e}_{j}\in\RR^{n}$ have $1$ in its $j^{\text{th}}$ component
and $0$ in all other components. Then the set
$\{\vec{e}_{1},\dots,\vec{e}_{n}\}$ forms a basis of $\RR^{n}$ and is
called its \define{Canonical Basis}.
\end{example}

\N{Vector spaces have many bases}
We see that a vector space may have more than one basis. In fact, they
will have many different possible bases (plural of basis). We saw one
basis in $\RR^{2}$ given by $\vec{z}=(1,1)$ and
$\bar{\vec{z}}=(1,-1)$. We also see there is the canonical basis for
$\RR^{2}$, which is different from the first basis.

The moral of the story is that we may have many inequivalent bases for
any given vector space.

\begin{lemma}
Let $V$ be a real vector space.
Let $B=\{\vec{v}_{1},\dots,\vec{v}_{n}\}$ form a basis for $V$.
Let $T=\{\vec{w}_{1},\dots,\vec{w}_{m}\}$ be a set of linearly
independent vectors from $V$.
Then $m\leq n$ (i.e., $|T|\leq|B|$).
\end{lemma}

\begin{proof}
Since $T$ consists of linearly independent vectors, we can write
$\vec{w}_{m}$ as a linear combination of basis vectors
\begin{equation}
\vec{w}_{m} = c^{(m)}_{1}\vec{b}_{1}+c^{(m)}_{2}\vec{b}_{2}+\cdots+c^{(m)}_{n}\vec{b}_{n}.
\end{equation}
We can reindex the basis vectors such that $c^{(m)}_{1}\neq0$. In that
case, we ``swap out'' $\vec{b}_{1}$ for $\vec{w}_{m}$ since we can write
$\vec{b}_{1}$ as a linear combination:
\begin{equation}
\vec{b}_{1}=\frac{1}{c^{(m)}_{1}}\vec{w}_{m} - \frac{c^{(m)}_{2}\vec{b}_{2}+\cdots+c^{(m)}_{n}\vec{b}_{n}}{c^{(m)}_{1}}.
\end{equation}
This gives us a new set of basis vectors $B_{1}$, and we consider $T_{1}=T\setminus\{\vec{w}_{m}\}$
the collection of elements from $T$ which are not $\vec{w}_{m}$. In
particular, there are $m-1$ elements of $T_{1}$.

We can reiterate this step, swapping one element out of $T_{1}$ and
putting it into $B_{1}$ (and throwing away an element from $B_{1}$ which
has been replaced) to produce a new basis $B_{2}$. We produce $T_{2}$
from $T_{1}$ by taking the remaining elements of $T_{1}$ which are not
in $B_{2}$ into $T_{2}$. We see $T_{2}$ has $m-2$ elemeents.

Eventually one of two possibilities occurs:
\begin{enumerate}
\item We'll reach $B_{k}$ which no longer has any original basis
  elements from $B$ in it --- they are disjoint $B\cap B_{k}=\emptyset$.
  But this would imply there are elements in $T_{k}$ which cannot be
  written as a linear combination of the basis, which is a
  contradiction; or
\item We'll exhaust $T_{k}$ and have no more elements from $T$ to add to $B_{k}$.
\end{enumerate}
We iterate this until we get to $B_{m}$ and $T_{m}$, because $T_{m+1}$
will be empty.
\end{proof}

\begin{theorem}[Any two bases have same number of elements]
Let $V$ be a real vector space. Suppose there exists at least one basis
$B$ for $V$, and suppose $B$ has finitely many element.
Then any two bases for $V$ have the same number of elements as each other.
\end{theorem}

\begin{proof}
Let $B_{1}$, $B_{2}$ be any two bases for $V$. Let $m=|B_{1}|$ and $n=|B_{2}|$.
We claim
\begin{enumerate}
\item $m\leq n$ by the previous lemma, and
\item $n\leq m$ by the previous lemma.
\end{enumerate}
Hence $m=n$.
\end{proof}

\begin{definition}
Let $V$ be a real vector space, let $B$ be a basis for $V$. If $B$ has
finitely many elements, then we say $V$ is
\define{Finite-Dimensional}. In that case, we call the number of vectors
in $B$ the \define{Dimension} of $V$.
\end{definition}

\begin{remark}
Reasoning about infinite-dimensional spaces can be tricky. We have
already seen one example, $\RR[x]$ the space of polynomials. The linear
algebra of inifnite-dimensional spaces usually goes by the name
``functional analysis''. A particularly friendly subfield is ``Fourier
analysis'', where many intuitions from finite-dimensional linear algebra
carries over.
\end{remark}

\begin{definition}
Let $V$ be a finite-dimensional vector space, let
$B=\{\vec{f}_{1},\dots,\vec{f}_{n}\}$ be a basis for $V$.
We define an \define{Ordered Basis} to be a tuple $(\vec{f}_{1},\dots,\vec{f}_{n})$.

Futhermore, if the vectors $\vec{f}_{1}$, \dots, $\vec{f}_{n}$ form a
basis such that
\begin{equation}
\vec{f}_{i}\cdot\vec{f}_{j}=0\quad\mbox{if }i\neq j,
\end{equation}
then we call it an \define{Orthogonal Basis} for $V$. If, even further,
we have
\begin{equation}
\vec{f}_{i}\cdot\vec{f}_{j}=\delta_{i,j}=\begin{cases}0&\mbox{if }i\neq j\\
1 & \mbox{if }i=j
\end{cases}
\end{equation}
then we call it an \define{Orthonormal Basis}.
\end{definition}

\begin{remark}
This might seem silly (and, I guess, it is), but sets are not
ordered. There are times when we will want to specifically note the
order of basis vectors. The ordering may be arbitrary (for example, an
accidental artifact induced by the indexing), but important.
\end{remark}

\begin{remark}
In differential geometry, we sometimes see the term ``frame'' used for
an ordered basis.
\end{remark}

\subsection{Coordinates Relative to a Basis}

\M
Recall, when we began discussing vector spaces like $\RR^{1}$ and
$\RR^{2}$ (back in section~\ref{section:vectors-in-r-n}), we began by
drawing a line, picking a point $O$ (calling it the origin), and then
picking another point $P$. We identified the oriented line segment
$\overrightarrow{OP}$ as a unit vector. Then any other point $Q$ could
be identified as a multiple of $\overrightarrow{OP}$ such that
$\|\overrightarrow{OQ}\|/\|\overrightarrow{OP}\|=|x|\in\RR$ and if $Q$
is not ``in the $P$ direction'', we said
$-|x|\overrightarrow{OP}=\overrightarrow{OQ}$. In this way, we
identified $x$ as the coordinate of $Q$.

\M In $\RR^{2}$, we did the same thing, but we now have two axes. Any
point $S$ on the plane could be identified by a pair of real numbers
$(x,y)\in\RR^{2}$ by similar means.

\M
In general, this is what happens with a vector in a vector space
relative to a basis. We obtain ``coordinates'' for the vector, enabling
us to write our vector as a linear combination of basis vectors. This is
a crucial point, because an $n$-dimensional real vector space $V$ \emph{is not}
\emph{identical} to $\RR^{n}$ --- but by choosing a basis, we can
identify vectors in $V$ with tuples of real numbers in $\RR^{n}$, namely
their coordinates. This should be familiar, we've been working with
$n\times1$ matrices and calling them vectors (but really, they're just
inhabitants of $\RR^{n}$). We have been stretching the truth all this
time.

\begin{ddanger}
This is a really critical point to appreciate: vectors are ``points in
the plane'' \emph{not} an $n\times 1$ matrix. \textbf{Vectors are not matrices}.
But we can turn a vector in a finite-dimensional vector space
\emph{into} a column $n\times1$ matrix, and vice-versa, \emph{given some basis}
for $V$.
\end{ddanger}

Let us try to make things concrete.

\begin{definition}\label{defn:basis:coordinates}
Let $V$ be a finite-dimensional vector space, let
$B=(\vec{f}_{1},\dots,\vec{f}_{n})$ be an ordered basis for $V$, and let
$\vec{v}\in V$ be an arbitrary vector. Then we call the coefficients
$\lambda_{1},\dots,\lambda_{n}\in\RR$ in
\begin{equation}
\vec{v} = \lambda_{1}\vec{f}_{1} + \cdots + \lambda_{n}\vec{f}_{n}
\end{equation}
the \define{Coordinates} of $\vec{v}$ over $B$ (or ``relative to $B$'').
We may write
\begin{equation}
[\vec{v}]_{B} = \begin{pmatrix}\lambda_{1}\\\vdots\\\lambda_{n}
\end{pmatrix}
\end{equation}
for the coordinates of $\vec{v}$ relative to basis $B$.
\end{definition}

\begin{remark}
If we can write $\vec{v} = \lambda_{1}\vec{f}_{1} + \lambda_{2}\vec{f}_{2} + \cdots + \lambda_{n}\vec{f}_{n}$,
then we can identify $\vec{v}$ with the column vector of its coordinates
relative to the $\vec{f}_{j}$:
\begin{equation}
  \vec{v} \mathrel{\mbox{``=''}}
  \begin{pmatrix}\lambda_{1}\\\lambda_{2}\\\vdots\\\lambda_{n}
  \end{pmatrix}.
\end{equation}
The equality is in quotation marks because it's not an equality but an \emph{isomorphism},
a slightly weaker notion of ``the same''.
We can prove (once we formalize the notion of an isomorphism) that any
finite-dimensional real vector space $V$ is isomorphic to $\RR^{n}$ the
collection of column $n$-vectors with real entries; this is done by
identifying a vector with its coordinates relative to some basis. 
But this \emph{does not} mean $V$ is equal to $\RR^{n}$.
\end{remark}

\begin{problem}
Suppose we have a finite-dimensional vector space $V$.
Suppose we have one ordered basis $B=(\vec{e}_{1},\dots,\vec{e}_{n})$
and a distinct ordered basis $B'=(\vec{f}_{1},\dots,\vec{f}_{n})$
which share no vectors --- we have $\vec{f}_{i}\neq\vec{e}_{j}$
for all $i$, $j=1,\dots,n$.

If we have a vector $\vec{v}\in V$ and have found its coordinates
relative to $\vec{f}_{j}$,
\begin{equation}
\vec{v} = \lambda_{1}\vec{f}_{1} + \cdots + \lambda_{n}\vec{f}_{n},
\end{equation}
then how do we transform these into coordinates relative to $\vec{e}_{i}$?
\end{problem}

\subsection{Changing Bases}

\N{Change of Basis Matrix}
Let $V$ be a finite-dimensional real vector space.
Let $B=(\vec{e}_{1},\dots,\vec{e}_{n})$ and
$C=(\vec{f}_{1},\dots,\vec{f}_{n})$ be two ordered bases for $V$.
Then the change of bases from $B$ to $C$ transforms coordinates for
vectors $[\vec{v}]_{B}$ according to the matrix
\begin{equation}
  \mat{M}^{B}_{C} = \left(\begin{array}{c|c|c}
      [\vec{e}_{1}]_{C} & \dots & [\vec{e}_{n}]_{C}
  \end{array}\right),
\end{equation}
so any vector $\vec{v}\in V$ expressed in coordinates $[\vec{v}]_{B}$
relative to $B$ may be expressed in coordinates relative to $C$ as
\begin{equation}
[\vec{v}]_{C} = \mat{M}^{B}_{C}[\vec{v}]_{B}.
\end{equation}
Why would this work? Well, if we expand the right-hand side, we would
find
\begin{equation}
 \left(\begin{array}{c|c|c}
      [\vec{e}_{1}]_{C} & \dots & [\vec{e}_{n}]_{C}\\
  \end{array}\right)\begin{pmatrix}\lambda_{1}\\\vdots\\\lambda_{n}
 \end{pmatrix} =
      \lambda_{1}[\vec{e}_{1}]_{C} + \lambda_{2}[\vec{e}_{2}]_{C} +
      \dots + \lambda_{n} [\vec{e}_{n}]_{C}.
\end{equation}
But since $[\vec{e}_{j}]_{C}$ is a linear combination of the basis
vectors $\vec{f}_{1}$, \dots, $\vec{f}_{n}$, this expands out to form a
linear combination of $\vec{f}_{1}$, \dots, $\vec{f}_{n}$.

\begin{theorem}
If $V$ is a finite dimensional vector space with ordered bases $B$ and
$C$, if $[\mat{M}]^{B}_{C}$ is the change-of-coordinate matrix from $B$
to $C$, then the change-of-coordinate matrix from $C$ back to $B$
(i.e., $[\mat{M}]^{C}_{B}$) satisfies
\begin{equation}
[\mat{M}]^{C}_{B} = ([\mat{M}]^{B}_{C})^{-1}.
\end{equation}
That is to say, they are inverses of each other.
\end{theorem}

This partly motivated the bizarre superscript/subscript convention, it
resembles a fraction.

We can ketch the proof out in (\S\ref{chunk:basis:change-of-basis-matrix-among-orthonormal-bases}).

\N{Relating Coordinates Between Orthonormal Bases}
If $B$ and $C$ are both orthonormal bases for $V$, there is no reason to
believe they consist of the same basis vectors. One way to obtain a
different set of orthonormal basis vectors from $B$ is by an arbitrary
rotation, and possibly reflection about an axis (or about a nonzero
vector). These transformations produce a different basis, but do not
affect the orthonormality of the new basis.

What does the change of coordinates matrix look like between them?

\M\label{chunk:basis:change-of-basis-matrix-among-orthonormal-bases}
We can write out the matrix components explicitly for orthonormal
coordinates $\vec{e}_{1}$, \dots, $\vec{e}_{n}$ and other orthonormal
coordinates $\vec{f}_{1}$, \dots, $\vec{f}_{n}$ as
\begin{align}
  \vec{f}_{1} &= (\vec{f}_{1}\cdot\vec{e}_{1})\vec{e}_{1} + (\vec{f}_{1}\cdot\vec{e}_{2})\vec{e}_{2} + \cdots + (\vec{f}_{1}\cdot\vec{e}_{n})\vec{e}_{n}\\
  \vec{f}_{2} &= (\vec{f}_{2}\cdot\vec{e}_{1})\vec{e}_{1} + (\vec{f}_{2}\cdot\vec{e}_{2})\vec{e}_{2} + \cdots + (\vec{f}_{2}\cdot\vec{e}_{n})\vec{e}_{n}\\
  \vdots &\mathrel{\phantom{=(\vec{f}_{2}\cdot\vec{e}_{1})}}\vdots\quad\phantom{+ (\vec{f}_{1}\cdot\vec{e}_{2})}\vdots\qquad\ddots\qquad\vdots\nonumber\\
  \vec{f}_{n} &= (\vec{f}_{n}\cdot\vec{e}_{1})\vec{e}_{1} + (\vec{f}_{n}\cdot\vec{e}_{2})\vec{e}_{2} + \cdots + (\vec{f}_{n}\cdot\vec{e}_{n})\vec{e}_{n}
\end{align}
The components of the matrix
$[\mat{M}]^{B}_{C}=(\vec{f}_{i}\cdot\vec{e}_{j})$.
What is the inverse of this matrix?

We could do some complicated math, or we could wonder the simpler
problem: what is $[\mat{M}]^{B}_{C}\transpose{([\mat{M}]^{B}_{C})}$?
\begin{calculation}
  ([\mat{M}]^{B}_{C}\transpose{([\mat{M}]^{B}_{C})})_{i,k}
\step{unfold the definition of matrix multiplication}
  \sum^{n}_{j=1}([\mat{M}]^{B}_{C})_{i,j}(\transpose{([\mat{M}]^{B}_{C})})_{j,k}
\step{unfold the definition of $[\mat{M}]^{B}_{C}$ into components}
  \sum^{n}_{j=1}(\transpose{\vec{f}}_{i}\vec{e}_{j})(\transpose{\vec{e}}_{j}\vec{f}_{k})
\step{distributivity}
  \transpose{\vec{f}}_{i}\left(\sum^{n}_{j=1}\vec{e}_{j}\transpose{\vec{e}}_{j}\right)\vec{f}_{k}
\step{matrix multiplication, see Lemma~\ref{lemma:basis:outer-product-of-orthonormal-basis} below}
  \transpose{\vec{f}}_{i}\left(\mat{I}_{n}\right)\vec{f}_{k}
\step{defining property of the identity matrix, associativity of multiplication}
  \transpose{\vec{f}}_{i}\vec{f}_{k}
\step{by definition of orthonormality}
  \delta_{i,k}
\end{calculation}
In other words,
\begin{equation}
[\mat{M}]^{B}_{C}\transpose{([\mat{M}]^{B}_{C})} = \mat{I}.
\end{equation}
This implies
\begin{equation}
([\mat{M}]^{B}_{C})^{-1} = \transpose{([\mat{M}]^{B}_{C})}.
\end{equation}
In other words, the change of basis matrix is an orthogonal matrix
(c.f., Exercise~\ref{xca:matrix-algebra:orthogonal-matrix}).

\begin{lemma}\label{lemma:basis:outer-product-of-orthonormal-basis}
Let $V$ be a finite-dimensional vector space, let $\vec{e}_{1}$, \dots,
$\vec{e}_{n}$ be an orthonormal basis for $V$. Then
\begin{equation}
\sum^{n}_{j=1}\vec{e}_{j}\transpose{\vec{e}}_{j} = \mat{I}_{n}.
\end{equation}
\end{lemma}
\begin{proof}
We consider how this acts on an arbitrary vector $\vec{v}\in V$.
\begin{calculation}
  \left(\sum^{n}_{j=1}\vec{e}_{j}\transpose{\vec{e}}_{j}\right)\vec{v}
\step{expanding $\vec{v}$ in the basis}
  \left(\sum^{n}_{j=1}\vec{e}_{j}\transpose{\vec{e}}_{j}\right)\left(\sum^{n}_{k=1}c_{k}\vec{e}_{k}\right)
\step{by linearity}
  \sum^{n}_{k=1}c_{k}\left(\sum^{n}_{j=1}\vec{e}_{j}\transpose{\vec{e}}_{j}\vec{e}_{k}\right)
\step{by definition of orthonormality}
  \sum^{n}_{k=1}c_{k}\left(\sum^{n}_{j=1}\vec{e}_{j}\delta_{j,k}\right)
\step{unrolling the inner sum over $j$}
  \sum^{n}_{k=1}c_{k}\left(\vec{e}_{1}\delta_{1,k}+\dots+\vec{e}_{k-1}\delta_{k-1,k}+\vec{e}_{k}\delta_{k,k}+\vec{e}_{k+1}\delta_{k+1,k}+\dots+\vec{e}_{n}\delta_{n,k}\right)
\step{definition of $\delta_{j,k}$}
  \sum^{n}_{k=1}c_{k}\left(\vec{e}_{1}0+\dots+\vec{e}_{k-1}0+\vec{e}_{k}1+\vec{e}_{k+1}0+\dots+\vec{e}_{n}0\right)
\step{arithmetic}
  \sum^{n}_{k=1}c_{k}\left(\vec{e}_{k}\right)
\step{multiplication}
  \sum^{n}_{k=1}c_{k}\vec{e}_{k}
\step{since this is the expansion of $\vec{v}$ in the basis $\vec{e}_{k}$, ``undoing'' the first step of this chain of calculations}
  \vec{v}.
\end{calculation}
Since this was for arbitrary $\vec{v}\in V$, it follows that
\begin{equation}
\sum^{n}_{j=1}\vec{e}_{j}\transpose{\vec{e}}_{j} = \mat{I}_{n},
\end{equation}
as desired.
\end{proof}

\subsection{Graham--Schmidt Method}

\N{Puzzle}
If we have an $n$-dimensional vector space $V$ with $n$ linearly
independent [nonzero] vectors $\vec{x}_{1}$, \dots, $\vec{x}_{n}$, then is there
any way to construct an orthonormal basis out of them?

\N{Solution}
We will construct an orthonormal basis, one vector at a time.

The first step is to construct our initial vector
\begin{equation}
\widehat{\vec{v}_{1}} = \frac{\vec{x}_{1}}{\|\vec{x}_{1}\|}.
\end{equation}
This is a unit vector, and now we will use it to start our collection.
We could have easily have chosen $\vec{v}_{1}=\vec{x}_{1}$, as well, it
would just make things a little longer.

We now find the second vector $\vec{v}_{2}$. Since $\vec{x}_{2}$ and
$\vec{x}_{1}$ are linearly independent, it follows that $\vec{x}_{2}$
and $\vec{v}_{1}$ are linearly independent. Then we hope to find
coefficients $c_{1}$ and $c_{2}$ such that
\begin{equation}
\vec{v}_{2} = c_{1}\widehat{\vec{v}_{1}} + c_{2}\vec{x}_{2}
\end{equation}
is a unit vector orthogonal to $\vec{v}_{1}$. So
\begin{equation}
\widehat{\vec{v}_{1}}\cdot\vec{v}_{2} = 0,
\end{equation}
which forces us to admit
\begin{subequations}
  \begin{align}
    0 &= \widehat{\vec{v}_{1}}\cdot(c_{1}\widehat{\vec{v}_{1}} + c_{2}\vec{x}_{2})\\
    &= c_{1}\widehat{\vec{v}_{1}}\cdot\widehat{\vec{v}_{1}} + c_{2}\widehat{\vec{v}_{1}}\cdot\vec{x}_{2}\\
    &=c_{1} + c_{2}\widehat{\vec{v}_{1}}\cdot\vec{x}_{2}
  \end{align}
  hence
  \begin{equation}
c_{1} = -c_{2}\widehat{\vec{v}_{1}}\cdot\vec{x}_{2}.
  \end{equation}
  Setting $c_{2}=1$ (since it's arbitrary), we find
  \begin{equation}
\vec{v}_{2} = -(\widehat{\vec{v}_{1}}\cdot\vec{x}_{2})\widehat{\vec{v}_{1}}+\vec{x}_{1}
=\vec{x}_{2} -(\widehat{\vec{v}_{1}}\cdot\vec{x}_{2})\widehat{\vec{v}_{1}}.
  \end{equation}
\end{subequations}
We can quickly check that $\widehat{\vec{v}_{1}}\cdot\vec{v}_{2}=0$.

We now can see that $\vec{v}_{1}$, $\vec{v}_{2}$ span everything which
$\vec{x}_{1}$, $\vec{x}_{2}$ spanned. Since $\vec{x}_{3}$ was
independent of $\vec{x}_{1}$ and $\vec{x}_{2}$, we have
$\vec{x}_{3}\notin\Span(\{\vec{x}_{1},\vec{x}_{2})$. Therefore we will
use $\vec{x}_{3}$ to construct $\vec{v}_{3}$ by writing
\begin{equation}
  \vec{v}_{3} = c_{1}\vec{v}_{1} + c_{2}\vec{v}_{2} + c_{3}\vec{x}_{3}.
\end{equation}
We are trying to determine the unknown coefficients $c_{1}$, $c_{2}$,
$c_{3}$. We know $\vec{v}_{3}$ will be orthogonal to $\vec{v}_{1}$ and
$\vec{v}_{2}$:
\begin{equation}
\vec{v}_{3}\cdot\vec{v}_{2}=0,\quad\mbox{and}\quad\vec{v}_{3}\cdot\vec{v}_{1}=0.
\end{equation}
The first of these give us (recalling $\vec{v}_{2}\cdot\vec{v}_{1}=0$),
\begin{equation}
\vec{v}_{3}\cdot\vec{v}_{2} = 0 = c_{2}\vec{v}_{2}\cdot\vec{v}_{2} + c_{3}\vec{x}_{3}\cdot\vec{v}_{2},
\end{equation}
hence
\begin{equation}
c_{2} = -c_{3}\frac{\vec{x}_{3}\cdot\vec{v}_{2}}{\vec{v}_{2}\cdot\vec{v}_{2}}.
\end{equation}
Similarly, we find
\begin{equation}
\vec{v}_{3}\cdot\vec{v}_{1} = 0 = c_{1}\vec{v}_{1}\cdot\vec{v}_{1} + c_{3}\vec{x}_{3}\cdot\vec{v}_{1},
\end{equation}
give us
\begin{equation}
c_{1} = -c_{3}\frac{\vec{x}_{3}\cdot\vec{v}_{1}}{\vec{v}_{1}\cdot\vec{v}_{1}}.
\end{equation}
Setting $c_{3}=1$ give us
\begin{equation}
\vec{v}_{3} = \vec{x}_{3} - \left(\frac{\vec{x}_{3}\cdot\vec{v}_{1}}{\vec{v}_{1}\cdot\vec{v}_{1}}\right)\vec{v}_{1}
-\left(\frac{\vec{x}_{3}\cdot\vec{v}_{2}}{\vec{v}_{2}\cdot\vec{v}_{2}}\right)\vec{v}_{2}.
\end{equation}
Now we have three basis vectors in our collection.

We see that
\begin{equation}
\vec{v}_{3} = \vec{x}_{3} - (\vec{x}_{3}\cdot\widehat{\vec{v}_{1}})\widehat{\vec{v}_{1}} - (\vec{x}_{3}\cdot\widehat{\vec{v}_{2}})\widehat{\vec{v}_{2}}.
\end{equation}
The general pattern seems to be
\begin{equation}
\vec{v}_{n+1} = \vec{x}_{n+1} - \sum^{n}_{k=1}\left(\frac{\vec{x}_{n+1}\cdot\vec{v}_{k}}{\vec{v}_{k}\cdot\vec{v}_{k}}\right)\vec{v}_{k}.
\end{equation}
Is this actually true?

Well, we've proven it works for $n=2$ and $n=3$, so we can try proving
it by induction. We assume this works for arbitrary $(n+1)\in\NN$. Then
the inductive case, supposing we have $n+1$ orthogonal vectors
$\vec{v}_{1}$, $\vec{v}_{2}$, \dots, $\vec{v}_{n+1}$, and we have a
vector $\vec{x}_{n+2}\notin\Span(\{\vec{v}_{1},\dots,\vec{v}_{n+1}\})$.
Then we claim
\begin{equation}
\vec{v}_{n+2} = \vec{x}_{n+2} - \sum^{n+1}_{k=1}\left(\frac{\vec{x}_{n+1}\cdot\vec{v}_{k}}{\vec{v}_{k}\cdot\vec{v}_{k}}\right)\vec{v}_{k}
\end{equation}
is orthogonal to $\vec{v}_{j}$ for $j=1,\dots,n+1$. To see this, we
compute,
\begin{calculation}
  \vec{v}_{j}\cdot\vec{v}_{n+2}
  \step{unfolding definition of $\vec{v}_{n+2}$}
  \vec{v}_{j}\cdot\left(\vec{x}_{n+2} - \sum^{n+1}_{k=1}\left(\frac{\vec{x}_{n+1}\cdot\vec{v}_{k}}{\vec{v}_{k}\cdot\vec{v}_{k}}\right)\vec{v}_{k}\right)
  \step{distributivity of dot product}
  \vec{v}_{j}\cdot\vec{x}_{n+2} - \sum^{n+1}_{k=1}\left(\frac{\vec{x}_{n+1}\cdot\vec{v}_{k}}{\vec{v}_{k}\cdot\vec{v}_{k}}\right)\vec{v}_{j}\cdot\vec{v}_{k}
  \step{orthogonality of $\vec{v}_{j}\cdot\vec{v}_{k}=\delta_{j,k}\vec{v}_{j}\cdot\vec{v}_{j}$}
  \vec{v}_{j}\cdot\vec{x}_{n+2} - \sum^{n+1}_{k=1}\left(\frac{\vec{x}_{n+1}\cdot\vec{v}_{k}}{\vec{v}_{k}\cdot\vec{v}_{k}}\right)\delta_{j,k}\vec{v}_{j}\cdot\vec{v}_{j}
  \step{defining property of $\delta_{j,k}$}
  \vec{v}_{j}\cdot\vec{x}_{n+2} - \left(\frac{\vec{x}_{n+1}\cdot\vec{v}_{j}}{\vec{v}_{j}\cdot\vec{v}_{j}}\right)\vec{v}_{j}\cdot\vec{v}_{j}
  \step{commutativity of multiplication}
  \vec{v}_{j}\cdot\vec{x}_{n+2} - \vec{x}_{n+1}\cdot\vec{v}_{j}\left(\frac{\vec{v}_{j}\cdot\vec{v}_{j}}{\vec{v}_{j}\cdot\vec{v}_{j}}\right)
  \step{since $\vec{v}_{j}\neq\vec{0}$ hence $\vec{v}_{j}\cdot\vec{v}_{j}\neq0$}
  \vec{v}_{j}\cdot\vec{x}_{n+2} - \vec{x}_{n+1}\cdot\vec{v}_{j}
  \step{arithmetic}
  0.
\end{calculation}
Hence $\vec{v}_{n+2}$ is orthogonal to $\vec{v}_{j}$ for every
$j=1,\dots,n+1$.

\N{Graham--Schmidt Algorithm}\label{chunk:graham-schmidt}
Given $m$ linearly independent nonzero vectors $\vec{x}_{1}$, \dots,
$\vec{x}_{m}$, we can construct an orthonormal basis for
$\Span(\{\vec{x}_{1},\dots,\vec{x}_{m}\})$ as follows:

\N*{Step 1.} Set $\vec{f}_{1}=\widehat{\vec{x}_{1}}$. Then go to step 2.

\N*{Step 2.} For each $\vec{x}_{2}$, \dots, $\vec{x}_{m}$, compute
$\vec{v}_{k}$ by
\begin{equation}
\vec{v}_{k} = \vec{x}_{k} - \sum^{k}_{j=1}(\vec{x}_{k}\cdot\vec{f}_{j})\vec{f}_{j},
\end{equation}
and then set
\begin{equation}
\vec{f}_{k} = \widehat{\vec{v}_{k}} = \frac{\vec{v}_{k}}{\|\vec{v}_{k}\|}.
\end{equation}
This produces an orthonormal basis $B=\{\vec{f}_{1},\dots,\vec{f}_{m}\}$.

\begin{remark}
If we have a subspace $U\subset V$ and have found an orthonormal basis
$B_{U}$ of $B$, then we can extend it to a basis $B$ of $V$ by taking
$n=\dim(V)$ linearly independent vectors in $V$, and applying the
Graham--Schmidt algorithm to add them to $B_{V}$.
\end{remark}

\begin{definition}\label{defn:basis:projection}
Let $\vec{u},\vec{v}\in V$ be vectors. Assume $\vec{u}\neq\vec{0}_{V}$.
We define the \define{Projection} of $\vec{v}$ along the $\vec{u}$
direction to be the vector
\begin{equation}
\operatorname{Proj}_{\vec{u}}(\vec{v}) = \frac{\vec{u}\cdot\vec{v}}{\vec{u}\cdot\vec{u}}\vec{u}=(\widehat{\vec{u}}\cdot\vec{v})\widehat{\vec{u}}
\end{equation}
where $\widehat{\vec{u}}=\vec{u}/\|\vec{u}\|$ is a unit vector.
\end{definition}

\begin{remark}
We can see that the Graham--Schmidt algorithm can be rephrased as taking
$n$ linearly independent vectors $\vec{x}_{1}$, $\vec{x}_{2}$, \dots,
$\vec{x}_{n}$, and forming
\begin{subequations}
\begin{align}
  \vec{u}_{1} &= \vec{x}_{1}\\
  \vec{u}_{2} &= \vec{x}_{2} - \operatorname{Proj}_{\vec{u}_{1}}(\vec{x}_{2})\\
  \vec{u}_{3} &= \vec{x}_{3} - \operatorname{Proj}_{\vec{u}_{1}}(\vec{x}_{3}) - \operatorname{Proj}_{\vec{u}_{2}}(\vec{x}_{3})\\
  \vdots \nonumber\\
  \vec{u}_{n} &= \vec{x}_{n} - \sum^{n-1}_{k=1}\operatorname{Proj}_{\vec{u}_{k}}(\vec{x}_{n}).
\end{align}
\end{subequations}
This produces a set of $n$ orthogonal vectors, and we could normalize
them (``put hats on them'') to obtain $n$ orthonormal vectors.
\end{remark}


\begin{definition}
Let $\vec{u},\vec{v}\in V$ be vectors. Assume $\vec{u}\neq\vec{0}_{V}$.
We define the \define{Orthogonal Decomposition} of $\vec{v}$ with
respect to $\vec{u}$ as consisting of two vectors:
\begin{enumerate}
\item the parallel part $\vec{v}^{\parallel} = \operatorname{Proj}_{\vec{u}}(\vec{v})$,
and
\item the perpendicular part $\vec{v}^{\perp} = \vec{v} - \vec{v}^{\parallel}$.
\end{enumerate}
\end{definition}
