\section{Matrix Algebra}

\M
Now, we want to define binary operators on matrices.
But we don't want these definitions to be arbitrary. We want:
\begin{enumerate}
\item Matrix multiplication produce a system of linear equations,
\item Matrix addition of vectors recovers the familiar operation in
  vector calculus.
\end{enumerate}

\begin{definition}
Let $\mat{A}=(a_{i,j})$ and $\mat{B}=(b_{i,j})$ be two $m\times n$ matrices.
We define \define{Matrix Addition} of $\mat{A}$ and $\mat{B}$ to produce a third
$m\times n$ matrix $\mat{C}=(c_{i,j})$ (called the \emph{sum} of $\mat{A}$ and $\mat{B}$)
whose components are defined by $c_{i,j}=a_{i,j}+b_{i,j}$.  

We write $\mat{A}+\mat{B}$ for the sum of $\mat{A}$ and $\mat{B}$.
\end{definition}

\begin{remark}
We should prove, for any $m\times n$ matrices $\mat{A}$ and $\mat{B}$, their sum
exists and is unique. This is ``obvious enough''.
\end{remark}

\begin{example}
  Let
  \begin{subequations}
\begin{equation}
  \mat{A} = \begin{bmatrix}-4 & -4 & 5\\
    3 & -1 & 4
  \end{bmatrix}
\end{equation}
and
\begin{equation}
\mat{B} = \begin{bmatrix}-3 & 1 & 2\\ 6 & -3 & -5
\end{bmatrix}.
\end{equation}
Then
\begin{equation}
\mat{A}+\mat{B} =\begin{bmatrix}-4-3 & -4+1 & 5+2\\
3+6 & -1-3 & 4-5
\end{bmatrix}=\begin{bmatrix}-7 & -3 & 7\\
9 & -4 & -1
\end{bmatrix}.
\end{equation}
  \end{subequations}
\end{example}

\begin{proposition}[Commutativity of Matrix Addition]
For any two $m\times n$ matrices $\mat{A}=(a_{i,j})$ and $\mat{B}=(b_{i,j})$, we
have
$\mat{A}+\mat{B}=\mat{B}+\mat{A}$.
\end{proposition}

\begin{proof}
We see $\mat{A}+\mat{B}=(a_{i,j}+b_{i,j})$ and for each component we have $a_{i,j}+b_{i,j}=b_{i,j}+a_{i,j}$
since componentwise we have addition of numbers (which is commutative).
But the matrix with $(b_{i,j}+a_{i,j})=\mat{B}+\mat{A}$.
Hence we find $\mat{A}+\mat{B}=\mat{B}+\mat{A}$.
\end{proof}

\begin{proposition}
For any $m\times n$ matrix $\mat{A}=(a_{i,j})$, the sum of $\mat{A}$ with the
$m\times n$ zero matrix $0$ is $\mat{A}$.
\end{proposition}

\begin{proof}
We see that the components of the zero matrix are all identically zero,
$\mat 0=[(0)_{i,j}]$. So
\begin{subequations}
\begin{calculation}
  \mat{A}+\mat{0}
\step{by definition of matrix addition}
   [a_{i,j}+(0)_{i,j}]
\step{since $(0)_{i,j} = 0$ for all $i$, $j$}
   [a_{i,j}+0]
\step{arithmetic}
   [a_{i,j}]
\step{definition of $\mat{A}$}
   \mat{A}.
\end{calculation}
\end{subequations}
Thus $\mat{A}+\mat{0}=\mat{A}$.
\end{proof}

\begin{definition}
Let $\mat{A}=(a_{i,j})$ be an $m\times n$ matrix, let $r$ be a real [or
complex] number.
We define the \define{Scalar Multiple} of $\mat{A}$ by $r$ to
be the $m\times n$ matrix $r\mat{A} = (ra_{i,j})$ whose components are
obtained by multiplying every component of $\mat{A}$ by $r$. We can also
write this as $r\cdot \mat{A}=r\mat{A}$ to make the scalar
multiplication explicit. 
\end{definition}

\begin{remark}
``Scalar multiplication'' refers to the fact we are multiplying a matrix
  by a ``scalar'' [number], as opposed to multiplying by a vector or
  another matrix.
\end{remark}

\begin{example}
Let $r$ be a positive integer, let $\mat{A}$ be any matrix. Then
\begin{equation}
\underbrace{\mat{A} + \dots + \mat{A}}_{r~\text{times}} = rA.
\end{equation}
We can see this by induction on $r$.

\textbf{Base case:} $r=1$ means $\mat{A} = 1\cdot\mat{A}$ which is
trivially true.

\textbf{Inductive Hypothesis:} we now assume for any positive integer
$r$ that $\mat{A}+\dots+\mat{A}=r\mat{A}$.

\textbf{Inductive Case:} we now prove that $r+1$ case.
We want to prove
\begin{equation}
\underbrace{\mat{A}+\dots+\mat{A}}_{r+1~\text{times}}=(r+1)\mat{A}.
\end{equation}
We can use the inductive hypothesis to write:
\begin{equation}
\underbrace{\mat{A}+\dots+\mat{A}}_{r~\text{times}}+\mat{A}=r\mat{A}+\mat{A}.
\end{equation}
Then we observe $r\mat{A} + \mat{A} = (ra_{i,j}) + (a_{i,j}) = ((r+1)a_{i,j})$
using the definition of matrix addition.
Then by definition of scalar multiplication this is precisely $(r+1)\mat{A}$.
\end{example}

\begin{remark}
We have a sort of consistency result between scalar multiplication and
adding a matrix to itself finitely many times. That's a good sign.
\end{remark}

\begin{example}
  Let $r=-2$ and
  \begin{subequations}
    \begin{equation}
\mat{A} = \begin{bmatrix}
  -2 & 6\\
  6 & -5
\end{bmatrix}.
    \end{equation}
    Then
    \begin{equation}
r\mat{A} =\begin{bmatrix}
  -2\cdot(-2) & -2\cdot6\\
  -2\cdot 6 & -2\cdot (-5)
\end{bmatrix} = \begin{bmatrix}
  4 & -12\\
  -12 & 10
\end{bmatrix}.
    \end{equation}
  \end{subequations}
\end{example}

\begin{example}
If $\mat{A}=(a_{i,j})$ is a scalar $n\times n$ matrix $\mat{A}=\diag(a,a,\dots,a)$, then
$\mat{A} = a\mat{I}_{n}$.
\end{example}

\N{Properties of the Scalar Product}
Let $r$, $s$ be numbers and $\mat{A}$, $\mat{B}$ be appropriately sized
matrices. Then the following hold:
\begin{enumerate}
\item $r(s\mat{A})=(rs)\mat{A}$
\item $(r+s)\mat{A}=r\mat{A}+s\mat{A}$ (distributivity)
\item $r(\mat{A}+\mat{B})=r\mat{A}+r\mat{B}$
\end{enumerate}

\begin{definition}
Let $\mat{A}=(a_{i,j})$, $\mat{B}=(b_{i,j})$ be two $m\times n$
matrices. We define their \define{Difference} to be the matrix $\mat{A}-\mat{B} = \mat{A} + -1\cdot\mat{B}$.
Similarly, the \define{Negation} of $\mat{A}$ is the matrix $-\mat{A}=-1\cdot\mat{A}$.
\end{definition}

\begin{proposition}
For any matrix $\mat{A}$, we have $\mat{A}-\mat{A}=\mat{0}$.
\end{proposition}

\phantomsection
\subsection*{Exercises}
\addcontentsline{toc}{subsection}{Exercises}

\begin{exercise}
Prove the sum of two diagonal matrices is another diagonal matrix. Is
this true for scalar matrices (the sum of two scalar matrices is a
scalar matrix)?
\end{exercise}

\subsection{Matrix Transpose}

\begin{definition}
Let $\mat{A}=(a_{i,j})$ be an $m\times n$ matrix.
We define the \define{Transpose} of $\mat A$ to be the $n\times m$ matrix $\transpose{\mat{A}}=(\transpose{a}_{j,i})$
where $\transpose{a}_{j,i}=a_{i,j}$.
\end{definition}

\begin{example}
  If we have
\begin{subequations}
\begin{equation}
\mat{A} = %\begin{bmatrix}
\left[\begin{array}{ccccc}
\rowcolor{olive!20}a_{1,1} & {\color{BrickRed}a_{1,2}} & {\color{DarkGreen}a_{1,3}} & \dots & a_{1,n}\\
a_{2,1} & a_{2,2} & a_{2,3} & \dots & a_{2,n}\\
a_{3,1} & a_{3,2} & a_{3,3} & \dots & a_{3,n}\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
a_{m,1} & a_{m,2} & a_{m,3} & \dots & a_{m,n}
  \end{array}\right],
%\end{bmatrix},
\end{equation}
(highlighting the first row to see where it goes, along with several
select entries), then we find
\begin{equation}
\transpose{\mat{A}} = %\begin{bmatrix}
\left[\begin{array}{>{\columncolor{olive!20}}ccccc}
a_{1,1} & a_{2,1} & a_{3,1} & \dots & a_{m,1}\\
{\color{BrickRed}a_{1,2}} & a_{2,2} & a_{3,2} & \dots & a_{m,2}\\
{\color{DarkGreen}a_{1,3}} & a_{2,3} & a_{3,3} & \dots & a_{m,3}\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
a_{1,n} & a_{2,n} & a_{3,n} & \dots & a_{m,n}
\end{array}\right].
%  \end{bmatrix}.
\end{equation}
\end{subequations}
\end{example}

\begin{proposition}[Transpose is idempotent]
For any matrix $\mat{A}$, we have $\transpose{(\transpose{\mat{A}})}=\mat{A}$.
\end{proposition}

\begin{proof}
Let $\mat{B}=\transpose{\mat{A}}$
Then $(\transpose{\mat{B}})_{i,j} = (\mat{B})_{j,i}$ by definition of
the transpose, and $(\mat{B})_{j,i} = (\transpose{\mat{A}})_{j,i} = (\mat{A})_{i,j}$.
Hence $\left(\transpose{(\transpose{\mat{A}})}\right)_{i,j} = (\mat{A})_{i,j}$,
as desired.
\end{proof}

\begin{definition}
Let $\mat{A}$ be an $n\times n$ matrix. We call $\mat{A}$
\define{Symmetric} if it is equal to its transpose: $\mat{A}=\transpose{\mat{A}}$.
Similarly, we call $\mat{A}$ \define{Antisymmetric} if it is the
negation of its transpose $\mat{A}=-\transpose{\mat{A}}$.
\end{definition}

\begin{example}
Let $\mat{A}$ be an anti-symmetric $3\times 3$ matrix. Then it must look
like
\begin{equation}
\mat{A} = \begin{pmatrix}0 & a_{1,2} & a_{2,3} \\
-a_{1,2} & 0 & -a_{3,1}\\
a_{3,1} & - a_{2,3} 0
\end{pmatrix}.
\end{equation}
In particular, the diagonal of an antisymmetric matrix consists of zero entries.
\end{example}

\phantomsection
\subsection*{Exercises}
\addcontentsline{toc}{subsection}{Exercises}

\begin{exercise}
If $\mat{U}$ is an upper triangular matrix, then is
$\transpose{\mat{U}}$ upper triangular or lower triangular?
\end{exercise}

\begin{exercise}
If $\mat{L}$ is an lower triangular matrix, then is
$\transpose{\mat{L}}$ upper triangular or lower triangular?
\end{exercise}

\begin{exercise}
Prove or find a counter-example: if $\mat{M}$ is an arbitrary $n\times n$
matrix, then $\mat{A} = (\mat{M} - \transpose{\mat{M}})/2$ is an
antisymmetric matrix and
$\mat{S} = (\mat{M} + \transpose{\mat{M}})/2$ is a symmetric matrix.
\end{exercise}

\begin{exercise}
Let $\mat{M}$ be an arbitrary $n\times n$ matrix.
Prove or find a counter-example: there exists a unique symmetric 
matrix $\mat{S}=\transpose{\mat{S}}$ and a unique antisymmetric matrix
$\mat{A}=-\transpose{\mat{A}}$ such that $\mat{M}=\mat{S}+\mat{A}$. 
\end{exercise}

\begin{exercise}
If $\mat{S}$ is a symmetric $n\times n$ matrix, is
$\mat{S}\transpose{\mat{S}}$ symmetric? What about $\mat{S}+\transpose{\mat{S}}$?
\end{exercise}

\subsection{Dot Product and Matrix Multiplication}

\begin{definition}
Recall, if we have two $n$-vectors $\vec{a}=(a_{1},\dots,a_{n})$ and
$\vec{b}=(b_{1},\dots,b_{n})$, we define their \define{Dot Product} to
be the scalar [number]
\begin{equation}
\vec{a}\cdot\vec{b} = a_{1}b_{1} + a_{2}b_{2} + \dots + a_{n}b_{n} = \sum^{n}_{j=1}a_{j}b_{j}.
\end{equation}
\end{definition}

\begin{example}
  Let
  \begin{equation}
\vec{a} = \begin{bmatrix}-1\\4\\-5
\end{bmatrix},\quad\mbox{and}\quad\vec{b}=\begin{bmatrix}
-6\\2\\4
\end{bmatrix}.
  \end{equation}
  Then
  \begin{equation}
    \begin{split}
    \vec{a}\cdot\vec{b} &= (-1)\cdot(-6) + 4\cdot2 + (-5)\cdot 4\\
    &= 6+8-20 = -6.
    \end{split}
  \end{equation}
\end{example}

\begin{proposition}
The dot product is symmetric, $\vec{a}\cdot\vec{b}=\vec{b}\cdot\vec{a}$.
\end{proposition}

\begin{definition}
Let $\mat{A}=(a_{i,j})$ be an $m\times p$ matrix,
let $\mat{B}=(b_{j,k})$ be an $p\times n$ matrix.
We define the \define{Matrix Multiplication} of $\mat{A}$ and $\mat{B}$
to be an $m\times n$ matrix $\mat{C}=(c_{i,k})$ whose components are
defined by the equation
\begin{equation}
c_{i,k} = \sum^{p}_{j=1}a_{i,j}b_{j,k}.
\end{equation}
That is to say, it is formed by taking the dot product of the
$i^{\text{th}}$ row of $\mat{A}$ with the $j^{\text{th}}$ column of $\mat{B}$,
\begin{equation*}
\left[\begin{array}{cccc}
a_{1,1} & a_{1,2} & \dots & a_{1,p}\\
a_{2,1} & a_{2,2} & \dots & a_{2,p}\\ 
\vdots & \vdots &   & \vdots\\
\rowcolor{olive!20}a_{i,1} & a_{i,2} & \dots & a_{i,p}\\
\vdots & \vdots &   & \vdots\\
a_{m,1} & a_{m,2} & \dots & a_{m,p}
  \end{array}\right]
\left[\begin{array}{ccc>{\columncolor{olive!20}}ccc}
b_{1,1} & b_{1,2} & \dots & b_{1,j} & \dots & b_{1,n}\\
b_{2,1} & b_{2,2} & \dots & b_{2,j} & \dots & b_{2,n}\\
\vdots & \vdots &       & \vdots  &       & \vdots\\
b_{p,1} & b_{p,2} & \dots & b_{p,j} & \dots & b_{p,n}
  \end{array}\right]
=
\left[\begin{array}{cccccc}
    c_{1,1} & c_{1,2} & \dots & c_{1,j} & \dots & c_{1,n}\\
    c_{2,1} & c_{2,2} & \dots & c_{2,j} & \dots & c_{2,n}\\
    \vdots & \vdots &       & \vdots  &       & \vdots\\
    c_{i,1} & c_{i,2} & \dots & {\colorbox{olive!20}{$c_{i,j}$}} & \dots & c_{i,n}\\
    \vdots & \vdots &        & \vdots &         & \vdots\\
    c_{m,1} & c_{m,2} & \dots & c_{m,j} & \dots & c_{m,n}
  \end{array}\right]
\end{equation*}
We denote the matrix multiplication of $\mat{A}$ and $\mat{B}$ by
$\mat{A}\mat{B}$. 
\end{definition}

\begin{example}
Let us consider an example just to show the ``mechanics'' of matrix
multiplication. Let
\begin{subequations}
\begin{equation}
\mat{A} = \begin{bmatrix}3 & 1 & 6\\2 & 1 & 3
\end{bmatrix}
\end{equation}
and
\begin{equation}
  \mat{B} = \begin{bmatrix}
    2 & 1 & 7\\
    6 & 1 & 0\\
    1 & -1 & 5
\end{bmatrix}.
\end{equation}
Then we find the component in the first column, first row, of the
product is:
\begin{equation}
  \begin{split}
  \mat{A}\mat{B} = \left[\begin{array}{ccc}\rowcolor{olive!20}3 & 1 & 6\\
      2 & 1 & 3
  \end{array}\right]\left[\begin{array}{>{\columncolor{olive!20}}ccc}
    2 & 1 & 7\\
    6 & 1 & 0\\
    1 & -1 & 5
    \end{array}\right]
  &= \begin{bmatrix}\colorbox{olive!20}{$3\cdot2 + 1\cdot6 + 6\cdot1$} & ? & ?\\
    ? & ? & ?
  \end{bmatrix}\\
  &= \begin{bmatrix}\colorbox{olive!20}{$18$} & ? & ?\\
    ? & ? & ?
  \end{bmatrix}
  \end{split}
\end{equation}
The next entry in the first row is:
\begin{equation}
  \begin{split}
  \mat{A}\mat{B} = \left[\begin{array}{ccc}\rowcolor{olive!20}3 & 1 & 6\\
      2 & 1 & 3
  \end{array}\right]\left[\begin{array}{c>{\columncolor{olive!20}}cc}
    2 & 1 & 7\\
    6 & 1 & 0\\
    1 & -1 & 5
    \end{array}\right]
  &= \begin{bmatrix}18 & \colorbox{olive!20}{$3\cdot1 + 1\cdot1 + 6\cdot-1$} & ?\\
    ? & ? & ?
  \end{bmatrix}\\
  &= \begin{bmatrix}18 & \colorbox{olive!20}{$-2$} & ?\\
    ? & ? & ?
  \end{bmatrix}
  \end{split}
\end{equation}
The last entry on the first row:
\begin{equation}
  \begin{split}
  \mat{A}\mat{B} = \left[\begin{array}{ccc}\rowcolor{olive!20}3 & 1 & 6\\
      2 & 1 & 3
  \end{array}\right]\left[\begin{array}{cc>{\columncolor{olive!20}}c}
    2 & 1 & 7\\
    6 & 1 & 0\\
    1 & -1 & 5
    \end{array}\right]
  &= \begin{bmatrix}18 & -2 & \colorbox{olive!20}{$3\cdot7 + 1\cdot0 + 6\cdot5$}\\
    ? & ? & ?
  \end{bmatrix}\\
  &= \begin{bmatrix}18 & -2 & \colorbox{olive!20}{$51$}\\
    ? & ? & ?
  \end{bmatrix}
  \end{split}
\end{equation}
We can continue to the second row:
\begin{equation}
  \begin{split}
  \mat{A}\mat{B} = \left[\begin{array}{ccc}3 & 1 & 6\\
\rowcolor{olive!20}2 & 1 & 3
  \end{array}\right]\left[\begin{array}{>{\columncolor{olive!20}}ccc}
    2 & 1 & 7\\
    6 & 1 & 0\\
    1 & -1 & 5
    \end{array}\right]
  &= \begin{bmatrix}18 & -2 & 51\\
    \colorbox{olive!20}{$2\cdot2 + 1\cdot6 + 3\cdot1$} & ? & ?
  \end{bmatrix}\\
  &= \begin{bmatrix}18 & -2 & 51\\
    \colorbox{olive!20}{$13$} & ? & ?
  \end{bmatrix}
  \end{split}
\end{equation}
The next column in the second row:
\begin{equation}
  \begin{split}
  \mat{A}\mat{B} = \left[\begin{array}{ccc}3 & 1 & 6\\
\rowcolor{olive!20}2 & 1 & 3
  \end{array}\right]\left[\begin{array}{c>{\columncolor{olive!20}}cc}
    2 & 1 & 7\\
    6 & 1 & 0\\
    1 & -1 & 5
    \end{array}\right]
  &= \begin{bmatrix}18 & -2 & 51\\
    13 & \colorbox{olive!20}{$2\cdot1 + 1\cdot1 + 3\cdot-1$} & ?
  \end{bmatrix}\\
  &= \begin{bmatrix}18 & -2 & 51\\
    13 & \colorbox{olive!20}{$0$} & ?
  \end{bmatrix}
  \end{split}
\end{equation}
Finally, the remaining entry:
\begin{equation}
  \begin{split}
  \mat{A}\mat{B} = \left[\begin{array}{ccc}3 & 1 & 6\\
\rowcolor{olive!20}2 & 1 & 3
  \end{array}\right]\left[\begin{array}{c>{\columncolor{olive!20}}cc}
    2 & 1 & 7\\
    6 & 1 & 0\\
    1 & -1 & 5
    \end{array}\right]
  &= \begin{bmatrix}18 & -2 & 51\\
    13 & 0 & \colorbox{olive!20}{$2\cdot7 + 1\cdot0 + 3\cdot5$}
  \end{bmatrix}\\
  &= \begin{bmatrix}18 & -2 & 51\\
    13 & 0 & \colorbox{olive!20}{$29$}
  \end{bmatrix}
  \end{split}
\end{equation}
\end{subequations}
\end{example}

\N{Recovering System of Linear Equations}
This is pretty random, why on Earth should we accept it?
Well, the biggest reason is because we recover a system of linear
equations by multiplying an $m\times n$ matrix by an $n\times 1$ column
vector.
If we write our $m\times n$ matrix $\mat{A}$ as $m$ row vectors
\begin{equation*}
  \mat{A} = \begin{bmatrix}\transpose{\vec{a}}_{1}\\
    \transpose{\vec{a}}_{2}\\
    \vdots\\
    \transpose{\vec{a}}_{m}
  \end{bmatrix}
\end{equation*}
then we see that multiplying it by our column vector $\vec{x}$ produces
\begin{equation}
  \mat{A}\vec{x} = \begin{bmatrix}\vec{a}_{1}\cdot\vec{x}\\
    \vec{a}_{2}\cdot\vec{x}\\
    \vdots\\
    \vec{a}_{m}\cdot\vec{x}
  \end{bmatrix}.
\end{equation}
If we have an $m$-vector of constants $\vec{b}$, and $\vec{x}$ were a
vector of unknowns, then
\begin{equation}
\mat{A}\vec{x} = \vec{b}
\end{equation}
is precisely a system of $m$ linear equations in $n$ unknowns. This is
wonderful: it's what we were trying to do all along!

\begin{example}
Let $\mat{A}=(a_{j,k})$ be an $m\times n$ matrix, consider the matrix
multiplication of the $m\times m$ identity matrix $\mat{I}_{m}$ with
$\mat{A}$. We find
\begin{calculation}
  (\mat{I}_{m}\mat{A})_{i,k}
\step{definition of matrix multiplication}
  \sum_{j=1}^{m}\delta_{i,j}a_{j,k}
  \step{breaking up the sum}
  \left(\sum_{j=1}^{i-1}\delta_{i,j}a_{j,k}\right)+\delta_{i,i}a_{i,k}+\left(\sum_{j=i+1}^{m}\delta_{i,j}a_{j,k}\right)
  \step{since $\delta_{i,j}=0$ if $i\neq j$}
  0 + \delta_{i,i}a_{i,k} + 0
\step{since $\delta_{i,i}=1$}
  a_{i,k}
\step{folding back $\mat{A}$ into the result}
  (\mat{A})_{i,k}.
\end{calculation}
Hence $\mat{I}_{m}\mat{A} = \mat{A}$.
\end{example}

\begin{example}
  Let
  \begin{subequations}
\begin{equation}
\mat{A} = \begin{bmatrix}1 & 2\\3 & 4
\end{bmatrix},
\end{equation}
and $\mat{I}_{2}$ be the 2-by-2 identity matrix. Then
\begin{equation}
\mat{A}\mat{I}_{2} = \mat{A}.
\end{equation}
  \end{subequations}
\end{example}

\begin{example}
Let $\mat{A}$ be a $1\times n$ matrix (i.e., a row $n$-vector) and
$\mat{B}$ be a $n\times 1$ matrix (i.e., a column $n$ vector). Let us
write $\mat{A}=(a_{1},\dots,a_{n})$ and $\mat{B}=\transpose{(b_{1},\dots,b_{n})}$.
Then
\begin{equation}
\mat{A}\mat{B} = \sum^{n}_{j=1}a_{j}b_{j}=a_{1}b_{1}+a_{2}b_{2}+\dots+a_{n}b_{n}.
\end{equation}
Does this look familiar? It's the dot product of two vectors $\vec{a}$,
$\vec{b}$. We specifically have, if $\vec{a}$ and $\vec{b}$ are both
column $n$-vectors, then
\begin{equation}
\vec{a}\cdot\vec{b} = \transpose{\vec{a}}\vec{b}.
\end{equation}
This relates matrix multiplication to the transpose and dot product.
\end{example}

\N{Rephrasing the definition}
If we have an $m\times n$ matrix $\mat{A}$ and an $n\times 2$ matrix
$\mat{B}$, we could view $\mat{B}$ as a pair of column $n$-vectors
\begin{equation}
\mat{B} = (\vec{b}_{1}, \vec{b}_{2}).
\end{equation}
In this case, we see that matrix multiplication amounts to,
\begin{equation}
\mat{A}\mat{B} = (\mat{A}\vec{b}_{1}, \mat{A}\vec{b}_{2}).
\end{equation}
We can continue in this manner, for an arbitrary $n\times p$ matrix
$\mat{C}$ thinking of it as $p$ column $n$-vectors
$\mat{C}=(\vec{c}_{1},\dots,\vec{c}_{p})$. Then matrix multiplication
amounts to
\begin{equation}
\mat{A}\mat{C} = (\mat{A}\vec{c}_{1}, \dots, \mat{A}\vec{c}_{p}).
\end{equation}
This is just rephrasing the definition more vividly.

\N{Concerns about the definition}
We should not get too ahead of ourselves here, we want to check matrix
multiplication has nice properties. Presumably not all the properties of
multiplying numbers, but hopefully enough of them. We should check for
associativity (very important property), commutativity (nice but
inessential), and distributivity over [matrix] addition.

\begin{theorem}[Associativity]
Let $\mat{A}$ be an $m\times n$ matrix, $\mat{B}$ be an $n\times p$
matrix, and $\mat{C}$ a $p\times q$ matrix.
Then matrix multiplication is associative, i.e.,
\begin{equation}
\mat{A}(\mat{B}\mat{C}) = (\mat{A}\mat{B})\mat{C}.
\end{equation}
\end{theorem}

There are two ways to prove this. The first is to define
$\mat{R}=\mat{B}\mat{C}$ and $\mat{L}=\mat{A}\mat{B}$, then unfold the
definitions to show $\mat{A}(\mat{B}\mat{C})=\mat{A}\mat{R}$ (by
definition of $\mat{R}$) and $(\mat{A}\mat{B})\mat{C}=\mat{L}\mat{C}$,
and then unfolding the definition of matrix multiplication we would
prove $\mat{A}\mat{R}=\mat{L}\mat{C}$. This involves rather nasty nested
sums.

The second approach is by induction on $q$. When $q=1$, we have
$\mat{C}$ be a column $p$-vector $\mat{C}=\vec{c}_{1}$. Matrix
multiplication becomes far more intuitive in this case. Proving
$\mat{A}(\mat{B}\vec{c}_{1})=(\mat{A}\mat{B})\vec{c}_{1}$ is the base
case of induction. Then we assume the inductive hypothesis (i.e., this
works for arbitrary $q$, that
$(\mat{A}\mat{B})\mat{C}_{q}=\mat{A}(\mat{B}\mat{C}_{q})$). Then we have
to prove the inductive case, when $\mat{C}=(\mat{C}_{q},\vec{c}_{q+1})$
is the block structure of $\mat{C}$. Intuitively this describes the
process of ``adding another column to $\mat{C}$, and proving
associativity still holds''. When combined with the base case, it
suffices to prove this works for any $q$.

\begin{remark}
One intuition of vectors is that they describe a certain state or
configuration, where each component refers to a different
degree-of-freedom. In Neo-Ricardian economics, this would be one
possible stock of goods (each component referring to a different commodity).
In physics, the components would be the coordinates for the positions of
various bodies.

Matrices then are used to transform states $\mat{A}\vec{x}_{\text{old}}=\vec{x}_{\text{new}}$. In Neo-Ricardian economics,
this is precisely the production process. In physics, this could be the
effect of rotation about an axis by a certain angle, or time-evolution
forward a certain amount of time.

Matrix multiplication describes composing these transformations (from
right to left), if $\mat{A}_{1}\vec{x}_{0}=\vec{x}_{1}$ describes how
$\mat{A}_{1}$ transforms $\vec{x}_{0}$, then
$\mat{A}_{2}\mat{A}_{1}\vec{x}_{0} = \mat{A}_{2}\vec{x}_{1}$. We could
use associativity to create a composite process
$\mat{A}_{\text{composite}}=\mat{A}_{2}\mat{A}_{1}$. But what's more, we
could create a composite process from any (finite number) of
intermediate processes, by matrix multiplication!
\end{remark}

\begin{example}[Matrix Multiplication is NOT commutative]
Lets compute
\begin{subequations}
\begin{equation}
\begin{bmatrix}1 & 1\\0 & 1 \end{bmatrix}
\begin{bmatrix}1 & 0\\1 & 1 \end{bmatrix}
=\begin{bmatrix}2 & 1\\1 & 1
\end{bmatrix}
\end{equation}
But at the same time, if we try commuting these two matrices, we get
\begin{equation}
\begin{bmatrix}1 & 0\\1 & 1 \end{bmatrix}
\begin{bmatrix}1 & 1\\0 & 1 \end{bmatrix}
=\begin{bmatrix}1 & 1\\1 & 2
\end{bmatrix}.
\end{equation}
\end{subequations}
Hence we conclude matrix multiplication is noncommutative, because we
have found a counter-example. And one counter-example is all we need to
disprove the hypothesis ``Matrix multiplication is commutative''.
\end{example}

\begin{proposition}
Let $r$ be a number, let $\mat{A}$ be an $m\times n$ matrix, let
$\mat{B}$ be an $n\times p$ matrix.
Then $\mat{A}(r\mat{B})=r(\mat{A}\mat{B})=(r\mat{A})\mat{B}$.
\end{proposition}

\begin{proposition}\label{prop:product-of-transpose}
Let $\mat{A}$ be an $m\times n$ matrix, let $\mat{B}$ be an $n\times p$ matrix.
Then the transpose of matrix product is the matrix multiplication of the
transposes, $\transpose{(\mat{A}\mat{B})} = \transpose{\mat{B}}\transpose{\mat{A}}$.
\end{proposition}

The proof will involve two steps: (1) expanding out
$(\transpose{(\mat{A}\mat{B})})_{k,i}$, and
(2) expanding out $(\transpose{\mat{B}}\transpose{\mat{A}})_{k,i}$.
Then we will find they are identical for every $i$, $k$.

\begin{proof}
  First we find
\begin{subequations}
\begin{calculation}
  (\transpose{\mat{B}}\transpose{\mat{A}})_{k,i}
\step*{definition of matrix multiplication}
  \sum_{j=1}^{n}(\transpose{\mat{B}})_{k,j}(\transpose{\mat{A}})_{j,i}
\step*{definition of transpose}
  \sum_{j=1}^{n}(\mat{B})_{j,k}(\mat{A})_{i,j}
\step{commutativity of multiplying components}
  \sum_{j=1}^{n}(\mat{A})_{i,j}(\mat{B})_{j,k}.
\end{calculation}

We similarly find, starting ``from the other end'' of the desired result,
\begin{calculation}
  \left(\transpose{(\mat{A}\mat{B})}\right)_{k,i}
\step*{definition of transpose}
  (\mat{A}\mat{B})_{i,k}
\step{definition of matrix multiplication}
  \sum^{n}_{j=1}(\mat{A})_{i,j}(\mat{B})_{j,k}.
\end{calculation}
But by comparing these two results, we find them identical, and since
it's true for every component of the product, we are forced to conclude
\begin{equation}
\transpose{(\mat{A}\mat{B})} = \transpose{\mat{B}}\transpose{\mat{A}}
\end{equation}
\end{subequations}
Hence the result.
\end{proof}

\begin{problem}
Is it true that $\transpose{(\mat{A}\mat{B}\mat{C})} = \transpose{\mat{C}}\transpose{\mat{B}}\transpose{\mat{A}}$?
What if we had $n$ factors, is it true that $\transpose{(\mat{A}_{1}\dots\mat{A}_{n})} = \transpose{\mat{A}}_{n}\dots\transpose{\mat{A}}_{1}$?
\end{problem}

\begin{example}[Plane geometry]\label{ex:matrix-algebra:plane-geometry}
We can look at $2\times2$ matrices as acting on points $(x,y)\in\RR^{2}$
on the plane. We turn $(x,y)$ into a column vector.

We have rotation anticlockwise by an angle
$\theta$ given by
\begin{equation}
\mat{A}_{\theta} = \begin{bmatrix}\cos(\theta) & -\sin(\theta)\\
\sin(\theta) & \cos(\theta)
\end{bmatrix}.
\end{equation}
Reflection about the $x$-axis is
\begin{equation}
\mat{R}  =\begin{bmatrix}1 & 0\\0 & -1
\end{bmatrix}.
\end{equation}
Dilation by a positive real number $\lambda>0$ is a diagonal matrix
\begin{equation}
  \mat{D}_{\lambda} =\begin{bmatrix}\lambda & 0\\
  0 & \lambda
  \end{bmatrix}.
\end{equation}
Observe for $0<\lambda<1$, $\mat{D}_{\lambda}$ is a contraction.
\end{example}

\begin{definition}
Let $n$ be a non-negative integer, let $\mat{A}$ be a square matrix.
We define the \define{Matrix Power} of $\mat{A}$ raised to the
$n^{\text{th}}$ power as the matrix $\mat{A}^{n}$ inductively defined by:
\begin{enumerate}
\item $\mat{A}^{0}=\mat{I}$, and
\item $\mat{A}^{n+1}=\mat{A}^{n}\mat{A}$ (and in particular $\mat{A}^{1}=\mat{A}$).
\end{enumerate}
So we have
\begin{equation}
\underbrace{\mat{A}\cdots\mat{A}}_{n~\text{times}}=\prod^{n}_{j=1}\mat{A}=\mat{A}^{n}.
\end{equation}
\end{definition}

\begin{example}
  Consider
  \begin{equation}
\mat{J} = \begin{bmatrix}0 & -1\\1 & 0
\end{bmatrix}.
  \end{equation}
  Then
  \begin{equation}
    \mat{J}^{2} = \begin{bmatrix}-1 & 0\\
      0 & -1
    \end{bmatrix} = -\mat{I}_{2}.
  \end{equation}
  Thus we have discovered some matrix which ``acts like'' $\sqrt{-1}$.
There is something profound here, if we examine Example~\ref{ex:matrix-algebra:plane-geometry},
we will find $J$ amounts to a rotation in $\RR^{2}$ anticlockwise by
$90^{\circ}$. This is precisely what happens if we multiply numbers in
the complex plane by $\I=\sqrt{-1}$.
\end{example}

\begin{example}
  The Fibonacci sequence is defined by $F_{0}=0$, $F_{1}=1$, and
  \begin{equation}
F_{n+1} = F_{n} + F_{n-1}.
  \end{equation}
  We see that
  \begin{equation}
F_{n+2} = F_{n+1} + F_{n} = (F_{n} + F_{n-1}) = 2F_{n}+F_{n-1}.
  \end{equation}
  Then we can describe it using a system of equations
  \begin{equation}
\begin{pmatrix}
1 & 1\\
1 & 2
\end{pmatrix}
\begin{pmatrix}F_{n-1}\\ F_{n}
\end{pmatrix} = \begin{pmatrix}F_{n+1}\\ F_{n+2}
\end{pmatrix}.
  \end{equation}
  We can use matrix power to simplify calculations to
  \begin{equation}
\begin{pmatrix}
1 & 1\\
1 & 2
\end{pmatrix}^{n}
\begin{pmatrix}0\\1
\end{pmatrix} = \begin{pmatrix}F_{n}\\ F_{n+1}
\end{pmatrix}.
\end{equation}
We will later find a really slick way to compute the powers of a matrix
quickly, because right now all we've done is rephrased the recurrence
relation in new notation.
\end{example}

\begin{problem}
  For any square matrix $\mat{A}$ and non-negative integers $p$ and $q$,
  prove $(\mat{A}^{p})^{q} = \mat{A}^{pq}$
  and $\mat{A}^{p}\mat{A}^{q}=\mat{A}^{p+q}$.
\end{problem}

\begin{problem}
Let $\displaystyle\mat{A}=\begin{bmatrix}1 & 1\\0 & 1 \end{bmatrix}$.
Compute $\mat{A}^{n}$. Start with $n=2$, $3$, $4$, then try to
generalize the results.
\end{problem}

\N{Puzzle}
Suppose we have an $m\times n$ matrix $\mat{A}$. When will there be an
$n\times m$ matrix $\mat{B}$ such that $\mat{B}\mat{A}=\mat{I}_{n}$? Or
$\mat{A}\mat{B}=\mat{I}_{m}$?

\phantomsection
\subsection*{Exercises}
\addcontentsline{toc}{subsection}{Exercises}

\begin{exercise}
  If you've never proven the geometric series formally, compute
  \[ (1 - x)\left(\sum^{\infty}_{n=0}x^{n}\right)\]
  formally (i.e., without worrying about convergence, just manipulate
  things algebraically, assuming the usual rules of arithmetic apply to variables).
\end{exercise}

\begin{exercise}
What is the computational complexity of matrix multiplication?
That is to say, if $\mat{A}$ is an $m\times n$ matrix and $\mat{B}$ is an
$n\times p$ matrix, how many addition and multiplication operations [of
  numbers] are needed to compute $\mat{A}\mat{B}$? [Hint: do this for
  the dot product of vectors, then do this for every entry in the result
  of the matrix multiplication.]
\end{exercise}

\begin{exercise}[Discussion question]
If we recall the Taylor series expansion for $\exp(x)$ from basic
calculus,
\[ \exp(x) = \sum^{\infty}_{n=0}\frac{x^{n}}{n!}, \]
could we pretend this works when $x$ is a square matrix? Why or why not?
Would this correspond to the limit definition of the exponential
function
\[ \exp(x) = \lim_{n\to\infty}\left(1 + \frac{x}{n}\right)^{n} \]
if we could replace $x$ with a square matrix?
\end{exercise}

\begin{exercise}[Left distributivity over addition]
Prove or find a counter-example: for any matrices [of suitable dimensions]
$\mat{A}$, $\mat{B}$, and $\mat{C}$, we have $\mat{A}(\mat{B}+\mat{C}=\mat{A}\mat{B}+\mat{A}\mat{C}$.
\end{exercise}

\begin{exercise}[Right distributivity over addition]
Prove or find a counter-example: for any matrices [of suitable dimensions]
$\mat{A}$, $\mat{B}$, and $\mat{C}$, we have $(\mat{A}+\mat{B})\mat{C}=\mat{A}\mat{C}+\mat{B}\mat{C}$.
\end{exercise}


\subsection{Matrix Inverse}

\begin{definition}
Let $\mat{A}$ be an $n\times n$ matrix.
We call an $n\times n$ matrix $\mat{B}$ an \define{Inverse} of $\mat{A}$
if
\begin{equation}
\mat{A}\mat{B}=\mat{B}\mat{A}=\mat{I}_{n}.
\end{equation}
In this case, we call $\mat{A}$ \define{Invertible} (or \define{Nonsingular}).

Otherwise, if no such $\mat{B}$ exists, then $\mat{A}$ is called a
\define{Noninvertible} (or \define{Singular}) matrix.
\end{definition}

\begin{example}
The identity matrix is its own inverse. The zero matrix has no inverse.
\end{example}

\begin{theorem}[Uniqueness of inverse matrix]
Let $\mat{A}$ be an $n\times n$ matrix. Assume $\mat{B}$ is the inverse
matrix for $\mat{A}$. Then $\mat{B}$ is unique.
\end{theorem}

By ``unique'', we mean if we happen to come across another inverse of
$\mat{A}$, then it will be equal (by matrix equality) to $\mat{B}$.

\begin{proof}
Let $\mat{B}_{1}$ and $\mat{B}_{2}$ be inverse matrices for $\mat{A}$. 
We will prove $\mat{B}_{1}=\mat{B}_{2}$.

Consider the following calculation:
\begin{subequations}
\begin{calculation}
  \mat{B}_{1}
\step{defining property of identity matrix}
  \mat{B}_{1}\mat{I}_{n}
\step{definition of inverse matrix}
  \mat{B}_{1}(\mat{A}\mat{B}_{2})
\step{associativity of matrix multiplication}
  (\mat{B}_{1}\mat{A})\mat{B}_{2}
\step{definition of matrix inverse}
  \mat{I}_{n}\mat{B}_{2}
\step{defining property of identity matrix}
  \mat{B}_{2}.
\end{calculation}
\end{subequations}
Hence we conclude $\mat{B}_{1}=\mat{B}_{2}$, as desired.
\end{proof}

\N{Notation for Inverse Matrix}
Since the inverse matrix is unique (if it exists), we will denote the
matrix inverse of $\mat{A}$ by $\mat{A}^{-1}$.

\begin{example}
  Consider the matrix
  \begin{equation}
\varepsilon = \begin{bmatrix}0 & 1\\
0 & 0
\end{bmatrix}.
  \end{equation}
  This is noninvertible. How can we see it? Well, we observe
  \begin{equation}\label{eq:example-of-nilpotent-matrix}
\varepsilon^{2}=\mat{0}.
  \end{equation}
  If $\varepsilon$ had an inverse matrix $\mat{A}$, then
\begin{subequations}
\begin{calculation}
  \varepsilon
\step{defining property of identity matrix}
  \mat{I}_{2}\varepsilon
\step{using the fact $\mat{A}$ is an inverse matrix}
  (\mat{A}\varepsilon)\varepsilon 
\step{associativity of matrix multiplication}
  \mat{A}(\varepsilon^{2})
\step{by Eq~\eqref{eq:example-of-nilpotent-matrix}}
  \mat{A}(\mat{0})
\step{defining property of zero matrix}
  \mat{0}.
\end{calculation}
\end{subequations}
Hence if $\varepsilon$ were invertible, we would have
$\varepsilon=\mat{0}$. But this contradicts the definition of $\varepsilon$,
which is a nonzero matrix. Thus we are forced to conclude $\varepsilon$
is noninvertible.
\end{example}

\N{Solving Systems of Linear Equations}\label{par:matrix-algebra:solving-systems-of-equations}
Returning to our original motivation for this diversion into matrix
algebra, we can now use matrix inversion to solve systems of linear
equations. We do it thus:\footnote{Since we are proving equations are
logically equivalent to each other, we write $(A=B)\equiv(A'=B')$ for
``$(A=B)$ is logically equivalent to $(A'=B')$, in the sense that one
implies the other \emph{and} vice-versa''.}
\begin{subequations}
\begin{calculation}\gdef\defaultrelation{\equiv}
  \mat{A}\vec{x} = \vec{b}
\step{multiply both sides by $\mat{A}^{-1}$}
  \mat{A}^{-1}(\mat{A}\vec{x}) = \mat{A}^{-1}\vec{b}
\step{associativity of matrix multiplication}
  (\mat{A}^{-1}\mat{A})\vec{x} = \mat{A}^{-1}\vec{b}
\step{definition of matrix inverse}
  \mat{I}_{n}\vec{x} = \mat{A}^{-1}\vec{b}
\step{defining property of identity matrix}
  \vec{x} = \mat{A}^{-1}\vec{b}
\end{calculation}
\end{subequations}
But we have just traded notation for notation: we currently have no
algorithm to compute the inverse of a matrix! While this is true, we can
still meaningfully prove properties about the matrix inverse (a useful
skill for mathematicians to know, how to prove things despite lacking an
algorithm for computing it). But we will address this problem in the
next couple sections.

\begin{theorem}[Inverse of products]
Let $\mat{A}$, $\mat{B}$ be invertible [square] matrices.
Then $(\mat{A}\mat{B})^{-1} = \mat{B}^{-1}\mat{A}^{-1}$.
\end{theorem}

\begin{proof}
  We compute directly
\begin{subequations}
\begin{calculation}\gdef\defaultrelation{\equiv}
  (\mat{A}\mat{B})^{-1}\mat{A}\mat{B} = \mat{I}
\step{multiply on right by $\mat{B}^{-1}$}
  (\mat{A}\mat{B})^{-1}\mat{A}\mat{B}\mat{B}^{-1} = \mat{I}\mat{B}^{-1}
\step{associativity of matrix multiplication, defining property of $\mat{I}$}
  (\mat{A}\mat{B})^{-1}\mat{A}(\mat{B}\mat{B}^{-1}) = \mat{B}^{-1}
\step{definition of matrix inverse of $\mat{B}$}
  (\mat{A}\mat{B})^{-1}\mat{A}\mat{I} = \mat{B}^{-1}
\step{defining property of identity matrix}
  (\mat{A}\mat{B})^{-1}\mat{A} = \mat{B}^{-1}
\step{multiply both sides on right by $\mat{A}^{-1}$}
  (\mat{A}\mat{B})^{-1}\mat{A}\mat{A}^{-1} = \mat{B}^{-1}\mat{A}^{-1}
\step{associativity of matrix multiplication}
  (\mat{A}\mat{B})^{-1}(\mat{A}\mat{A}^{-1}) = \mat{B}^{-1}\mat{A}^{-1}
\step{definition of matrix inverse}
  (\mat{A}\mat{B})^{-1}\mat{I} = \mat{B}^{-1}\mat{A}^{-1}
\step{defining property of identity matrix}
  (\mat{A}\mat{B})^{-1} = \mat{B}^{-1}\mat{A}^{-1}
\end{calculation}
\end{subequations}
Hence we obtain the desired result.
\end{proof}

\begin{proposition}[Inverse commutes with transpose]
  Let $\mat{A}$ be an invertible matrix. Then
  $\transpose{(\mat{A}^{-1})} = (\transpose{\mat{A}})^{-1}$.
\end{proposition}

\begin{proof}
Recall Proposition~\ref{prop:product-of-transpose} which established the
product of the transpose is the product-in-reverse-order of transposes
$\transpose{(\mat{A}\mat{B})} = \transpose{\mat{B}}\transpose{\mat{A}}$.
Set $\mat{B}=\mat{A}^{-1}$.
We find the left hand side becomes,
\begin{equation}
\transpose{(\mat{A}\mat{B})} = \transpose{(\mat{A}\mat{A}^{-1})} = \transpose{\mat{I}}=\mat{I};
\end{equation}
the right-hand side becomes
\begin{equation}
\transpose{\mat{B}}\transpose{\mat{A}} = \transpose{(\mat{A}^{-1})}\transpose{\mat{A}}.
\end{equation}
Setting these equal yields
\begin{subequations}
\begin{calculation}\gdef\defaultrelation{\equiv}
  \transpose{(\mat{A}^{-1})}\transpose{\mat{A}} = \mat{I}
\step{multiply both sides on the right by $(\transpose{\mat{A}})^{-1}$}
  \transpose{(\mat{A}^{-1})}\transpose{\mat{A}}(\transpose{\mat{A}})^{-1} = \mat{I}(\transpose{\mat{A}})^{-1}
\step{associativity of matrix multiplication, defining property of identity matrix}
  \transpose{(\mat{A}^{-1})}(\transpose{\mat{A}}(\transpose{\mat{A}})^{-1}) = (\transpose{\mat{A}})^{-1}
\step{definition of matrix inverse}
  \transpose{(\mat{A}^{-1})}\mat{I} = (\transpose{\mat{A}})^{-1}
\step{defining property of identity matrix}
  \transpose{(\mat{A}^{-1})} = (\transpose{\mat{A}})^{-1}
\end{calculation}
\end{subequations}
Hence we obtain the desired result.
\end{proof}

\begin{proposition}[Matrix inversion is idempotent]
Let $\mat{A}$ be an invertible matrix. Then $(\mat{A}^{-1})^{-1}=\mat{A}$
\end{proposition}

\begin{proof}
Let $\mat{B}=\mat{A}^{-1}$. Then $\mat{A}\mat{B}=\mat{B}\mat{A}=\mat{I}$.
But this implies $\mat{A} = \mat{B}^{-1} = (\mat{A}^{-1})^{-1}$, as desired.
\end{proof}


\begin{definition}
Let $\mat{A}\vec{x}=\vec{b}$ be a system of equations.
We call a solution \define{Trivial} if $\vec{x}=\vec{0}$ it is the zero vector.
If a solution is not the zero vector, then we call it a
\define{Nontrivial} solution.
\end{definition}

\begin{theorem}
Let $\mat{A}$ be an $n\times n$ matrix.
The system of equations $\mat{A}\vec{x}=\vec{0}$
has a nontrivial solution if and only if $\mat{A}$ is singular [i.e.,
  noninvertible]. 
\end{theorem}

This statement is of the form ``$p$ if and only if $q$'', so there are
two claims being made: (1) ``if $p$, then $q$'', and (2) ``if $q$, then $p$''.
We will need to prove both claims.

\begin{proof}
$(\implies)$ We want to prove the claim ``if $\mat{A}\vec{x}=\vec{0}$
  has a nontrivial solution, then $\mat{A}$ is noninvertible.''
  We can prove the contrapositive --- that is, we recognize ``if $p$,
  then $q$'' is logically equivalent to ``if not-$q$, then not-$p$''.
  We assume that $\mat{A}$ is invertible, and we will show
  $\mat{A}\vec{x}=\vec{0}$ has only trivial solutions. We can see this
  by multiplying both sides on the left by $\mat{A}^{-1}$, which gives
  us the system $\mat{A}^{-1}\mat{A}\vec{x} = \mat{A}^{-1}\vec{0}$.
  The right-hand side is still the zero vector. The left-hand side is
  just $\vec{x}$. Hence we deduce $\vec{x}=\vec{0}$ --- only the trivial
  solutions satisfy the system.

$(\impliedby)$ We need to prove if $\mat{A}$ is singular, then
  $\mat{A}\vec{x}=\vec{0}$ has a nontrivial solution. Logically, this
  looks like ``$p\implies q$'', which is logically equivalent to ``$(\neg p)\lor{q}$''
  [either not-$p$, or $q$, or both]
  and ``$\neg(p\land\neg{q})$'' [not ($p$ and not-$q$)].
  We will prove this by contradiction: we will prove ``$p\land\neg{q}$
  implies a contradiction''.

  So assume that $\mat{A}$ is invertible \emph{and}
  $\mat{A}\vec{x}=\vec{0}$ has a nontrivial solution. But then we just
  proved $\mat{A}\vec{x}=\vec{0}$ implies $\mat{A}$ is noninvertible. We
  therefore conclude that $\mat{A}$ is both invertible and
  noninvertible, a contradiction.
\end{proof}

\begin{remark}
Proofs by contradiction always feel lackluster. Did we, you know,
\emph{do anything}? This feeling is normal.
\end{remark}

\N{Logically equivalent conditions to invertibility}
Let $\mat{A}$ be an $n\times n$ matrix. The following are logically
equivalent to each other:
\begin{enumerate}
\item $\mat{A}$ is invertible
\item $\vec{x}=\vec{0}$ is the only solution to $\mat{A}\vec{x}=\vec{0}$
\item $\mat{A}\vec{x}=\vec{b}$ has a unique solution for every column
  $n$-vector $\vec{b}$.
\end{enumerate}

\phantomsection
\subsection*{Exercises}
\addcontentsline{toc}{subsection}{Exercises}

\begin{exercise}
Prove or find a counter-example: if $\mat{P}$ is an invertible $n\times n$
matrix and $\mat{A}$ is an $n\times n$ matrix, then for any $k\in\NN$ we
have $(\mat{P}\mat{A}\mat{P}^{-1})^{k} = \mat{P}\mat{A}^{k}\mat{P}^{-1}$.
\end{exercise}

\begin{exercise}
Let $\mat{U}$ be a strictly upper-triangular $n\times n$ matrix (i.e.,
$\mat{U}$ is upper-triangular and its diagonal entries are zero).
\begin{enumerate}
\item Prove $\mat{U}^{n}=0$
\item Prove $(\mat{I}+\mat{U})^{-1} = \sum^{n}_{k=0}\mat{U}^{k}$
\end{enumerate}
\end{exercise}

\begin{exercise}
Let $\mat{A}$ be an invertible $n\times n$ matrix, let $\mat{B}$ be an
arbitrary $n\times n$ matrix, and let $0<\varepsilon\ll1$.
Prove or find a counter-example:
$(\mat{A}+\varepsilon\mat{B})^{-1} = \mat{A}^{-1}(1 + \varepsilon\mat{A}^{-1}\mat{B})^{-1}$.
\end{exercise}

\begin{exercise}
We call a square matrix $\mat{A}$ \define{Orthogonal} if its transpose
is its inverse $\mat{A}^{-1}=\transpose{\mat{A}}$.
\begin{enumerate}
\item Is the identity matrix orthogonal?
\item If $\mat{A}$ is an orthogonal $n\times n$ matrix, then [letting
  $0<\varepsilon\ll1$ be ``infinitesimal'', i.e., we ignore all terms of
  order $\varepsilon^{2}$ but keep terms of first-order in $\varepsilon$]
  when will $\mat{A}+\varepsilon\mat{B}$ be an orthogonal matrix?
\end{enumerate}
\end{exercise}