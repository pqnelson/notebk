\section{Differentiating Functions of Grassmann Algebras}

\M
Suppose we have a Grassmann algebra
$\Lambda[\theta_{1},\dots,\theta_{n}]=\Lambda^{m,n}$ over some field
$\FF$ with bosonic generators $x_{1}$, \dots, $x_{m}$ and canonical odd
generators $\theta_{1}$, \dots, $\theta_{n}$.
For any function $f\colon\Lambda^{m,n}\to\Lambda^{m,n}$, we know it may
be written as:
\begin{equation}\label{eq:diff:expansion}
\begin{split}
  &f(x_{1},\dots,x_{m};\theta_{1},\dots,\theta_{n})\\
&=f_{0}(x) + \sum_{i}f^{(1)}_{i}(x)\theta_{i} + \sum_{i<j}f^{(2)}_{ij}(x)\theta_{i}\theta_{j}+\dots
+f^{(n)}_{12\dots n}(x)\theta_{1}\theta_{2}\dots\theta_{n}.
\end{split}
\end{equation}
We could define
the left derivative with respect to $\theta_{k}$ by:
\begin{enumerate}[label={\textsc{Step} \arabic*.}]
\item Moving $\theta_{k}$ to the left in each term in the expansion Eq~\eqref{eq:diff:expansion}
  and write this as
%  \begin{equation}
\begin{multline}
f(x_{1},\dots,x_{m};\theta_{1},\dots,\theta_{k},\dots,\theta_{n})\\
=f_{0}(x) + f^{(1)}_{k}\theta_{k} + \sum_{i\neq k}f^{(1)}_{i}(x)\theta_{i}
+ \sum_{i<k}f^{(2)}_{ik}\theta_{i}\theta_{k} + \sum_{k<j}f^{(2)}_{kj}\theta_{k}\theta_{j}
+ \dots\\
+ (-1)^{k-1}f^{(n)}_{12\dots n}\theta_{k}\theta_{1}\dots\theta_{k-1}\theta_{k+1}\dots\theta_{n}.
\end{multline}
%  \end{equation}
\item Consider, for any arbitrary odd variable $\varepsilon\propto\theta_{k}$,
\begin{multline}
f(x;\theta_{1},\dots,\theta_{k}+\varepsilon,\dots,\theta_{n})\\
= f(x;\theta_{1},\dots,\theta_{k},\dots,\theta_{n})
 + \varepsilon
 g(x;\underbrace{\theta_{1},\dots,\theta_{k-1},\theta_{k+1},\dots,\theta_{n}}_{n-1~\text{odd variables}}).
\end{multline}
\item We identify
  $g(x;\theta_{1},\dots,\theta_{k-1},\theta_{k+1},\dots,\theta_{n})$ as
  the left derivative of $f$ with respect to $\theta_{k}$
\end{enumerate}
We can define the right derivative of $f$ with respect to $\theta_{k}$
analogously, just move everything to the right, then we expand
$f(x;\theta+\varepsilon)=f(x;\theta)+h(x;\theta_{1},\dots,\theta_{k-1},\theta_{k+1},\dots,\theta_{n})\varepsilon$.
We identify
$h(x;\theta_{1},\dots,\theta_{k-1},\theta_{k+1},\dots,\theta_{n})$ as
the right-derivative of $f$ with respect to $\theta_{k}$.

In short, since every term of $f$ looks like some function of the
bosonic variables $x$ multiplied by a monomial of the odd variables, we
just move all instances of $\theta_{k}$ to the left (respectively, to the right), throw
away all terms which do not contain a factor of $\theta_{k}$, then
discard the $\theta_{k}$ factor. The result is the left (respectively,
right) derivative of $f$ with respect to $\theta_{k}$.

\begin{puzzle}
What rules of normal differentiation hold for differentiation with
respect to Grassmann generators? The most important rule are the product
rule and chain-rule, we can derive everything else from these two rules.
\end{puzzle}

\begin{exercise}
Let $\theta_{1}$, $\theta_{2}$ be generators for $\Lambda^{2,2}$.
If we take the quantities
\begin{equation}
\phi_{\pm} = \frac{\theta_{1}\pm\theta_{2}}{\sqrt{2}},
\end{equation}
then what is the derivative
\begin{equation*}
\frac{\partial}{\partial\theta_{i}}(\phi_{+}\phi_{-})
\end{equation*}
equal to?
\end{exercise}

\begin{exercise}[Chain-rule lives!]
  Let $a_{ij}$ be an $n\times n$ matrix of real numbers (or elements
  from $\FF$), and let $\theta_{1}$, \dots, $\theta_{n}$ be generators
  for $\Lambda^{m,n}$. Take
\begin{subequations}
  \begin{equation}
z_{i} = \sum_{j}a_{ij}\theta_{j},\quad f(z)=f(z(\theta)).
  \end{equation}
  Then prove or find a counter-example:
  \begin{align}
    \frac{\overrightarrow{\partial}}{\partial\theta_{i}}f[z(\theta)] &\stackrel{???}{=} \sum_{i}\left.\frac{\overrightarrow{\partial}}{\partial z_{i}}f(z)\right|_{z=z(\theta)}a_{ij},\\
    f[z(\theta)]\frac{\overleftarrow{\partial}}{\partial\theta_{i}} &\stackrel{???}{=} \sum_{i}\left.f(z)\frac{\overleftarrow{\partial}}{\partial z_{i}}\right|_{z=z(\theta)}a_{ij},
  \end{align}
\end{subequations}
where the arrow indicates whether we use left differentiation (arrow
points to the left) or right differentiation (arrow points to the right).
\end{exercise}

\begin{exercise}
Prove or find a counter-example: Let $\theta_{1}$, $\theta_{2}$ be
generators for $\Lambda^{2,2}$, and let $f\in\Lambda^{2,2}$ be
arbitrary. Then the derivatives with respect to the Grassmann generators commute:
\begin{equation}
\frac{\partial}{\partial\theta_{2}}\frac{\partial}{\partial\theta_{1}}f
\stackrel{???}{=}\frac{\partial}{\partial\theta_{1}}\frac{\partial}{\partial\theta_{2}}f
\end{equation}
\end{exercise}


\begin{puzzle}
For $f\in\Lambda^{1,1}$, does it makes sense to ask what is the
derivative of $f$ with respect to $x+\theta$? What could it be?
\end{puzzle}

\begin{exercise}
Prove that Taylor expansion about 0 holds for any $f\in\Lambda^{m,n}$, which
will produce something like
\begin{equation}
f(x,\theta) = f(x)\sum^{n}_{\ell=0}f_{i_{1}\dots i_{\ell}}(x)\theta^{i_{1}}(\cdots)\theta^{i_{\ell}}
\end{equation}
where $f(x)=f(x,0)$ is obtained by setting all Grassmann generators to
zero (assuming that $f(x,0)$ has a Taylor expansion near $0$ --- what
happens if we choose to Taylor expand about a different point?).
\end{exercise}


\begin{exercise}\label{xca:differentiation:taylor-series-for-exp}
For any $\alpha\in\Lambda^{m,n}$ which looks like
$\alpha=f(x)+\beta(x,\theta)$ (where all generators of the
Grassmann algebra appear in $\beta$). We can formally define exponential
function using the Taylor series expansion,
\begin{equation}
\exp(\alpha) = \E^{f}\left(1 + \beta + \frac{\beta^{2}}{2!} + \dots + \frac{\beta^{n}}{n!}\right).
\end{equation}
Prove this works. What laws does it obey? Do we expect, for example, $\exp(\alpha+\gamma)=\exp(\alpha)\exp(\gamma)$?
Is $\exp(-\alpha)=1/\exp(\alpha)$?
\end{exercise}

\begin{exercise}
  Let $f\in\Lambda^{m,n}$.
  \begin{enumerate}
  \item Determine the trigonometric functions $\cos(f)$ and $\sin(f)$ in terms
of the trigonometric functions of the body and soul $\cos(f_{B})$,
$\cos(f_{S})$, $\sin(f_{B})$, and $\sin(f_{S})$.
\item Do we get the same result if we use the Taylor expansion, as opposed to
the identity $\exp(\I x)=\cos(x)+\I\sin(x)$? [I mean, we just computed
  $\exp(f)$ in the previous exercise, it's natural to wonder if we can
  use it to determine trigonometric functions, like we usually do.]
\item Do the angle-addition identities still hold?
  \end{enumerate}
\end{exercise}


\begin{exercise}
Let $f\in\Lambda^{m,n}$, let $\theta_{j}$ be some generator for the
Grassmann algebra. Is it still true that
\begin{equation}
  \frac{\partial}{\partial\theta_{j}}\exp\left(f(x,\theta)\right)
  \stackrel{???}{=} \frac{\partial f}{\partial\theta_{j}}\exp\left(f(x,\theta)\right)
\end{equation}
or not? For what $f$ can we take \emph{two} derivatives with respect to
$\theta_{j}$ and obtain a nonzero quantity? If such an $f$ exists, are
there any which permit taking $k\in\NN$ derivatives with respect to
$\theta_{j}$ and obtaining a nonzero quantity?
\end{exercise}

\begin{exercise}[A.~Schwarz 2008]
  Compute
  \begin{equation}
\frac{\partial}{\partial\theta_{4}}\log(1 + \theta_{1}\theta_{2} + \theta_{3}\theta_{4}
+ \theta_{1}\theta_{2}\theta_{3}\theta_{4})
  \end{equation}
in two ways: using the chain rule and directly.
\end{exercise}
